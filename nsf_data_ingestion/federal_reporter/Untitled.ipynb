{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from shutil import rmtree\n",
    "import os\n",
    "from shutil import copyfile\n",
    "from shutil import rmtree\n",
    "import tarfile\n",
    "import shutil\n",
    "import zipfile\n",
    "from ftplib import FTP\n",
    "import glob, os, os.path\n",
    "import logging\n",
    "import calendar\n",
    "import time\n",
    "from subprocess import call\n",
    "sys.path.append('/home/eileen/nsf_data_ingestion/')\n",
    "from nsf_data_ingestion.config import nsf_config\n",
    "from nsf_data_ingestion.objects import data_source_params\n",
    "from nsf_data_ingestion.utils.utils_functions import get_last_load\n",
    "\n",
    "#  FEDERAL DOWNLOAD FUNCTIONS\n",
    "######################################################################################################################################################################\n",
    "def download_fed_data(param_list):\n",
    "    logging.info(\"the list of parameters are \", param_list)\n",
    "#     directory_path_data = param_list.get('directory_path')\n",
    "#     timestamp_file = param_list.get('timestamp_file')\n",
    "\n",
    "    federal_directory_path_data = param_list.get('directory_path')\n",
    "    timestamp_file = param_list.get('timestamp_file')\n",
    "    #directory_path_data ='/home/eileen/federal_data/'\n",
    "    #timestamp_file = 'time_stamp.txt'\n",
    "    last_load = get_last_load(federal_directory_path_data, timestamp_file)\n",
    "#     if last_load >= 604800:\n",
    "    if last_load >= 8400000:\n",
    "        if os.path.exists(federal_directory_path_data):\n",
    "            rmtree(federal_directory_path_data)\n",
    "\n",
    "        os.makedirs(federal_directory_path_data)\n",
    "        for i in range(2004, 2017):\n",
    "            logging.info('Downloading Fed Data.......')\n",
    "            os.system(\n",
    "                'wget ' + param_list.get('fed_FedRePORTER_PRJ_url') +str(i)+ '.zip -nv -P ' + federal_directory_path_data)\n",
    "            os.system(\n",
    "                'wget ' + param_list.get('fed_FedRePORTER_PRJABS_url') +str(i)+ '.zip -nv -P ' + federal_directory_path_data)\n",
    "        logging.info('Downloading Complete.........')\n",
    "        \n",
    "        logging.info('Updating TimeStamp........')\n",
    "        f = open(federal_directory_path_data + \"time_stamp.txt\", \"a\")\n",
    "        cur_time = calendar.timegm(time.gmtime())\n",
    "        f.write(str(cur_time))\n",
    "        f.close()\n",
    "        logging.info('Download Complete........')\n",
    "        \n",
    "    else:\n",
    "        logging.info('Data Intact......!!!!!')\n",
    "        \n",
    "\n",
    "def persist1(param_list):\n",
    "    logging.info(\"the list of parameters are \", param_list)\n",
    "    federal_directory_path_data = param_list.get('directory_path')\n",
    "    federal_xml_path= param_list.get('xml_path')\n",
    "#     data_path = param_list.get('directory_path')\n",
    "#     hdfs_path = param_list.get('hdfs_path')\n",
    "    #data_path ='/home/eileen/federal_data/'\n",
    "    #hdfs_path = '/user/eileen/federal/xml/'\n",
    "    logging.info(\"data path eileen \" , federal_directory_path_data)\n",
    "    filelist = glob.glob(os.path.join(federal_directory_path_data, \"*.xml\"))\n",
    "#     for f in filelist:\n",
    "#         os.remove(f)\n",
    "    logging.info('Persisting data to HDFS', call([\"hdfs\", \"dfs\", \"-test\", \"-d\", federal_directory_path_data]))\n",
    "    logging.info('the data path is persists : ',federal_directory_path_data)\n",
    "#     os.system('jar -cvf' + data_path + '\"*FedRePORTER_PRJ_X_FY*.zip\" > '+ data_path+ 'projects.xml')\n",
    "# #     os.system('unzip -p ' + data_path + '\"*FedRePORTER_PRJABS_X_FY*.zip\" >'+ data_path+'abstracts.xml')\n",
    "# #     logging.info('Persisting data to HDFS')\n",
    "# #     if not call([\"hdfs\", \"dfs\", \"-test\", \"-d\", hdfs_path]):\n",
    "# #         call([\"hdfs\", \"dfs\", \"-rm\", \"-r\", \"-f\", hdfs_path])\n",
    "        \n",
    "# #     call([\"hdfs\", \"dfs\", \"-mkdir\", hdfs_path])\n",
    "    logging.info('Persisting FedRePORTER_PRJ..............')\n",
    "    call('unzip -p \"'+federal_directory_path_data+'*FedRePORTER_PRJ_X_FY*.zip\"'+' | hdfs dfs -put - '+federal_directory_path_data+'projects.xml', shell = True)\n",
    "    logging.info('Persisting FedRePORTER_PRJABS..............')\n",
    "    call('unzip -p \"'+federal_directory_path_data+'*FedRePORTER_PRJABS_X_FY*.zip\"'+' | hdfs dfs -put - '+federal_directory_path_data+'abstracts.xml', shell = True)\n",
    "    logging.info('Data Persisted..............')\n",
    "    logging.info('Persisting data to HDFS')\n",
    "    if not call([\"hdfs\", \"dfs\", \"-test\", \"-d\", federal_xml_path]):\n",
    "        call([\"hdfs\", \"dfs\", \"-rm\", \"-r\", \"-f\", federal_xml_path])\n",
    "        \n",
    "    call([\"hdfs\", \"dfs\", \"-mkdir\", federal_xml_path])\n",
    "    logging.info('Persisting FedRePORTER_PRJ..............')\n",
    "    call([\"hdfs\", \"dfs\", \"-put\", federal_directory_path_data+'projects.xml', federal_xml_path])\n",
    "    \n",
    "    logging.info('Persisting FedRePORTER_PRJABS..............')\n",
    "    call([\"hdfs\", \"dfs\", \"-put\", federal_directory_path_data+'abstracts.xml', federal_xml_path])\n",
    "    logging.info('Data Persisted..............')\n",
    "    \n",
    "    \n",
    "def persist(param_list):\n",
    "    federal_directory_path_data = param_list.get('directory_path')\n",
    "    federal_xml_path= param_list.get('xml_path')\n",
    "    #data_path = '/home/eileen/federal_data/'\n",
    "    #grants_gov_xml_path = '/user/eileen/federal/xml/'\n",
    "    logging.info('Persisting data to HDFS', call([\"hdfs\", \"dfs\", \"-test\", \"-d\", federal_xml_path]))\n",
    "        \n",
    "    if not call([\"hdfs\", \"dfs\", \"-test\", \"-d\", federal_xml_path]):\n",
    "         call([\"hdfs\", \"dfs\", \"-rm\",\"-r\",\"-f\", federal_xml_path])\n",
    "    call([\"hdfs\", \"dfs\", \"-mkdir\", federal_xml_path])\n",
    "    call('unzip -p \"'+federal_directory_path_data+'DownloadFile?fileToDownload=FedRePORTER_PRJ_X*.zip\"'+' | hdfs dfs -put -   '+federal_xml_path+'projects.xml', shell = True)\n",
    "    call('unzip -p \"'+federal_directory_path_data+'DownloadFile?fileToDownload=FedRePORTER_PRJABS_X_FY*.zip\"'+' | hdfs dfs -put -   '+federal_xml_path+'abstracts.xml', shell = True)\n",
    "\n",
    "\n",
    "def download(data_source_name):\n",
    "    download_fed_data(data_source_params.mapping.get(data_source_name))\n",
    "\n",
    "def persist_hdfs(data_source_name):\n",
    "    persist(data_source_params.mapping.get(data_source_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/opt/cloudera/parcels/SPARK2-2.3.0.cloudera3-1.cdh5.13.3.p0.458809/lib/spark2/')\n",
    "import sys\n",
    "import zipfile\n",
    "import io\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "import os\n",
    "from os import path\n",
    "from shutil import copyfile\n",
    "from shutil import rmtree\n",
    "from subprocess import call\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def convert_to_parquet(spark, project_folder):\n",
    "    file_list = ['projects', 'abstracts']\n",
    "    for filename in file_list:\n",
    "        location = project_folder + filename\n",
    "        df = spark.read.\\\n",
    "            format('com.databricks.spark.xml').\\\n",
    "            options(rowTag='ROW').\\\n",
    "            load(location + '.xml')\n",
    "        \n",
    "        if not call([\"hdfs\", \"dfs\", \"-test\", \"-d\", location +'.parquet']):\n",
    "            logging.info('Parquet Files Exist Deleting .......')\n",
    "            call([\"hdfs\", \"dfs\", \"-rm\", \"-r\", \"-f\", location +'.parquet'])\n",
    "            \n",
    "        logging.info('Writing New parquet Files .......')\n",
    "        df.write.parquet(location + '.parquet')\n",
    "##\n",
    "def main(data_source_name): \n",
    "    project_folder = '/user/eileen/federal/xml/'\n",
    "    logging.info('Creating Spark Session....')\n",
    "    spark = SparkSession.builder.config('spark.jars', '/home/eileen/nsf_data_ingestion/libraries/spark-xml_2.11-0.5.0.jar').config(\"spark.executor.instances\", '3').config(\"spark.executor.memory\", '10g').config('spark.executor.cores', '3').config('spark.cores.max', '3').appName(data_source_name).getOrCreate()\n",
    "    logging.info('Writing to Parquet.....')\n",
    "    convert_to_parquet(spark, project_folder)\n",
    "    logging.info('Parquet Write Complete.....')\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Creating Spark Session....\n"
     ]
    }
   ],
   "source": [
    "main(nsf_config.federal_reporter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
