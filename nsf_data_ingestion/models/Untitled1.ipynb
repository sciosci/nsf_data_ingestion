{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/ananth/nsf_data_ingestion/')\n",
    "from nsf_data_ingestion.config import nsf_config\n",
    "from nsf_data_ingestion.objects import data_source_params\n",
    "import os\n",
    "import findspark\n",
    "findspark.init('/opt/cloudera/parcels/SPARK2-2.3.0.cloudera3-1.cdh5.13.3.p0.458809/lib/spark2/')\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import functions as fn\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.ml.feature import CountVectorizer, IDF, Tokenizer, RegexTokenizer, StopWordsRemover, IDF\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import Row\n",
    "\n",
    "import requests\n",
    "# getting stop words\n",
    "stop_words = requests.get(nsf_config.stop_words_url).text.split()\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "def create_spark_session(name):\n",
    "    spark = SparkSession.builder.config(\"spark.executor.memory\", '30g')\\\n",
    "    .config('spark.executor.cores', '7')\\\n",
    "    .config('spark.cores.max', '7')\\\n",
    "    .appName('tfdf')\\\n",
    "    .getOrCreate()\n",
    "    return spark\n",
    "\n",
    "def read_medline(spark, processed_path):\n",
    "    \"\"\"Creates a dataframe with the columns:\n",
    "    `id`: global id\n",
    "    `source`: medline\n",
    "    `source_id`: PMID\n",
    "    `type`: publication\n",
    "    `title`\n",
    "    `venue`: journal\n",
    "    `abstract`\n",
    "    `scientists`: authors\n",
    "    `organizations`: affiliation\n",
    "    `date`: publication date\n",
    "    `content`: concatenation of abstract, affiliation, author, and journal\n",
    "    \"\"\"\n",
    "    medline_path = processed_path\n",
    "    medline_df = spark.read.parquet(medline_path)\n",
    "    return medline_df.select(\n",
    "            fn.concat(fn.lit('medline_'), fn.col('pmid')).alias('id'),\n",
    "            fn.lit('medline').alias('source'),\n",
    "            fn.col('pmid').astype('string').alias('source_id'),\n",
    "            fn.lit('publication').alias('type'),\n",
    "            'title',\n",
    "            fn.col('journal').alias('venue'),\n",
    "            'abstract',\n",
    "            fn.col('author').alias('scientists'),\n",
    "            fn.col('affiliation').alias('organizations'),\n",
    "            fn.col('pubdate').alias('date'),\n",
    "            fn.concat_ws(' ',\n",
    "                      fn.col('abstract'),\n",
    "                      fn.col('affiliation'),\n",
    "                      fn.col('author'),\n",
    "                      fn.col('journal')).alias('content'),\n",
    "            fn.lit(None).astype('string').alias('end_date'),\n",
    "            fn.lit(None).astype('string').alias('city'),\n",
    "            fn.lit(None).astype('string').alias('country'),\n",
    "            fn.lit(None).astype('string').alias('other_id')\n",
    "    )\n",
    "\n",
    "def read_federal_exporter(spark, processed_path):\n",
    "    \"\"\"Creates a dataframe with the columns:\n",
    "    `id`: global id\n",
    "    `source`: federal_exporter\n",
    "    `source_id`: project id\n",
    "    `type`: grant\n",
    "    `title`: PROJECT_TITLE\n",
    "    `venue`: AGENCY\n",
    "    `abstract`\n",
    "    `scientists`: CONTACT_PI + OTHER_PIS\n",
    "    `organizations`: ORGANIZATION_NAME\n",
    "    `date`: BUDGET_START_DATE\n",
    "    `content`: concatenation of abstract, title, PIs, agency, and organization name\n",
    "    `end_date`: BUDGET_END_DATE\n",
    "    `city`: ORGANIZATION_CITY\n",
    "    `country`: ORGANIZATION_COUNTRY\n",
    "    `other_id`: PROJECT_NUMBER\n",
    "    \"\"\"\n",
    "    abstracts_df = spark.read.parquet(os.path.join(processed_path, 'abstracts.parquet'))\n",
    "    projects_df = spark.read.parquet(os.path.join(processed_path, 'projects.parquet'))\n",
    "    together_df = projects_df.join(abstracts_df, 'PROJECT_ID')\n",
    "    return together_df.select(fn.concat(fn.lit('fe_'), fn.col('PROJECT_ID')).alias('id'),\n",
    "        fn.lit('federal_exporter').alias('source'),\n",
    "        fn.col('PROJECT_ID').astype('string').alias('source_id'),\n",
    "        fn.lit('grant').alias('type'),\n",
    "        fn.col('PROJECT_TITLE').alias('title'),\n",
    "        fn.col('AGENCY').alias('venue'),\n",
    "        fn.col('ABSTRACT').alias('abstract'),\n",
    "        fn.concat_ws('; ',\n",
    "                     fn.col('CONTACT_PI_PROJECT_LEADER'),\n",
    "                     fn.col('OTHER_PIS')).alias('scientists'),\n",
    "        fn.col('ORGANIZATION_NAME').astype('string').alias('organizations'),\n",
    "        fn.col('BUDGET_START_DATE').alias('date'),\n",
    "        fn.concat_ws(' ',\n",
    "                  fn.col('ABSTRACT'),\n",
    "                  fn.col('PROJECT_TITLE'),\n",
    "                  fn.concat_ws(' ', fn.col('CONTACT_PI_PROJECT_LEADER'), fn.col('OTHER_PIS')),\n",
    "                  fn.col('AGENCY'),\n",
    "                  fn.col('ORGANIZATION_NAME')\n",
    "            ).alias('content'),\n",
    "        fn.col('BUDGET_END_DATE').alias('end_date'),\n",
    "        fn.col('ORGANIZATION_CITY').alias('city'),\n",
    "        fn.col('ORGANIZATION_COUNTRY').alias('country'),\n",
    "        fn.col('PROJECT_NUMBER').alias('other_id')\n",
    ")\n",
    "\n",
    "def read_arxiv(spark, processed_path):\n",
    "    \"\"\"Creates a dataframe with the columns:\n",
    "    `id`: global id\n",
    "    `source`: arxiv\n",
    "    `source_id`: arxiv id\n",
    "    `type`: publication\n",
    "    `title`\n",
    "    `venue`: concatenation of subjects\n",
    "    `abstract`\n",
    "    `scientists`: authors\n",
    "    `organizations`: null\n",
    "    `date`: publication date\n",
    "    `content`: concatenation of abstract, affiliation, author, and journal\n",
    "    \"\"\"\n",
    "    arxiv_path = os.path.join(processed_path)\n",
    "    arxiv_df = spark.read.parquet(arxiv_path)\n",
    "    return arxiv_df.select(\n",
    "        fn.concat(fn.lit('arxiv_'), fn.col('id')).alias('id'),\n",
    "        fn.lit('arxiv').alias('source'),\n",
    "        fn.col('id').astype('string').alias('source_id'),\n",
    "        fn.lit('publication').alias('type'),\n",
    "        'title',\n",
    "        fn.concat_ws('; ', 'subjects').alias('venue'),\n",
    "        'abstract',\n",
    "        fn.concat_ws(';', 'authors').alias('scientists'),\n",
    "        fn.lit(None).astype('string').alias('organizations'),\n",
    "        fn.col('datastamp').alias('date'),\n",
    "        fn.concat_ws(' ',\n",
    "                  fn.col('abstract'),\n",
    "                  fn.col('title'),\n",
    "                  fn.concat_ws(' ', 'authors'),\n",
    "                  fn.concat_ws(' ', 'subjects')).alias('content'),\n",
    "        fn.lit(None).astype('string').alias('end_date'),\n",
    "        fn.lit(None).astype('string').alias('city'),\n",
    "        fn.lit(None).astype('string').alias('country'),\n",
    "        fn.lit(None).astype('string').alias('other_id')\n",
    "    )\n",
    "\n",
    "\n",
    "def add_rowid(x):\n",
    "    \"\"\"Called on a RDD when zipWithIndex() is used\"\"\"\n",
    "    return Row(row_id = x[1], **x[0].asDict())\n",
    "\n",
    "def fit_tfidf_pipeline(content_df):\n",
    "    tokenizer = RegexTokenizer(). \\\n",
    "        setGaps(False). \\\n",
    "        setPattern('\\\\p{L}+'). \\\n",
    "        setInputCol('content'). \\\n",
    "        setOutputCol('words')\n",
    "\n",
    "    sw = StopWordsRemover() \\\n",
    "        .setStopWords(stop_words) \\\n",
    "        .setCaseSensitive(False) \\\n",
    "        .setInputCol(\"words\") \\\n",
    "        .setOutputCol(\"filtered\")\n",
    "\n",
    "    cv = CountVectorizer(). \\\n",
    "        setInputCol('filtered'). \\\n",
    "        setOutputCol('tf'). \\\n",
    "        setMinTF(1). \\\n",
    "        setMinDF(10). \\\n",
    "        setVocabSize(2 ** 17)\n",
    "\n",
    "    # fit dataframe_df\n",
    "    cv_transformer = Pipeline(stages=[tokenizer, sw, cv]).fit(content_df)\n",
    "\n",
    "    idf = IDF(minDocFreq=10). \\\n",
    "        setInputCol('tf'). \\\n",
    "        setOutputCol('tfidf')\n",
    "\n",
    "    tfidf_transformer = Pipeline(stages=[cv_transformer, idf]).fit(content_df)\n",
    "\n",
    "    return tfidf_transformer\n",
    "\n",
    "def main(data_source):\n",
    "    processed_path = '/user/ananth/medline/parquet/'\n",
    "    fed_processed_path = '/user/ananth/data/raw/federal_exporter/'\n",
    "    models_path = '/user/ananth/tdif/'\n",
    "    tfidf_path = '/user/ananth/tdifupdate/'\n",
    "    arxiv_path = '/user/ananth/arxiv/parquet/'\n",
    "    spark = create_spark_session('tfdf')\n",
    "\n",
    "    arxiv_df = read_arxiv(spark, arxiv_path)\n",
    "    medline_df = read_medline(spark, processed_path)\n",
    "    fe_df = read_federal_exporter(spark, fed_processed_path)\n",
    "\n",
    "    dataframe_list = [medline_df, fe_df]\n",
    "    all_data_df = content_df = reduce(DataFrame.unionAll, dataframe_list)\n",
    "\n",
    "    tfidf_transformer = fit_tfidf_pipeline(all_data_df)\n",
    "    tfidf_model_path = os.path.join(models_path, 'tfidf_transformer.model')\n",
    "\n",
    "    tfidf_transformer.write().overwrite().save(tfidf_model_path)\n",
    "    tfidf_df = tfidf_transformer.transform(all_data_df). \\\n",
    "            select(all_data_df.columns + ['tfidf'])\n",
    "    tfidf_df.write.parquet(tfidf_path, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main('tfdif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
