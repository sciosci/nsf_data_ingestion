{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.lsimodel import Projection\n",
    "import findspark\n",
    "findspark.init('/opt/cloudera/parcels/SPARK2-2.3.0.cloudera3-1.cdh5.13.3.p0.458809/lib/spark2/')\n",
    "import pyspark\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql import functions as fn\n",
    "from pyspark.sql import SparkSession\n",
    "from subprocess import call\n",
    "from pyspark.ml.linalg import VectorUDT\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "def create_spark_session():\n",
    "    logging.info('Creating Spark Session.....')\n",
    "    spark = SparkSession.builder.config(\"spark.executor.instances\", '3')\\\n",
    "    .config(\"spark.executor.memory\", '30g')\\\n",
    "    .config('spark.executor.cores', '7')\\\n",
    "    .config('spark.cores.max', '7')\\\n",
    "    .config('spark.kryoserializer.buffer.max.mb', '2000')\\\n",
    "    .appName(\"svd-test\")\\\n",
    "    .getOrCreate()\n",
    "    logging.info('Spark Session Created.....')\n",
    "    \n",
    "    logging.info('Adding Libraries....')\n",
    "    spark.sparkContext.addPyFile('/home/eileen/nsf_data_ingestion/libraries/gensim.zip')\n",
    "    spark.sparkContext.addPyFile('/home/eileen/nsf_data_ingestion/libraries/boto3.zip')\n",
    "    spark.sparkContext.addPyFile('/home/eileen/nsf_data_ingestion/libraries/botocore.zip')\n",
    "    spark.sparkContext.addPyFile('/home/eileen/nsf_data_ingestion/libraries/jmespath.zip')\n",
    "    spark.sparkContext.addPyFile('/home/eileen/nsf_data_ingestion/libraries/smart_open.zip')\n",
    "    spark.sparkContext.addPyFile('/home/ananth/nsf_data_ingestion/dist/nsf_data_ingestion-0.0.1-py3.6.egg')\n",
    "    return spark\n",
    "\n",
    "\n",
    "def create_projection(m, k, docs, power_iters=2, extra_dims=10):\n",
    "    yield Projection(m, k, docs=docs, use_svdlibc=False, power_iters=power_iters, extra_dims=extra_dims)\n",
    "\n",
    "\n",
    "def merge(p1, p2, decay=1.):\n",
    "    p1.merge(p2, decay=decay)\n",
    "    return p1\n",
    "\n",
    "\n",
    "def binary_aggregate(rdd, f):\n",
    "    \"\"\"Aggregate rdd using function f in a binary tree.\n",
    "    By definition, it will return an RDD with one partition\n",
    "    \"\"\"\n",
    "\n",
    "    zeroValue = None, True\n",
    "\n",
    "    def op(x, y):\n",
    "        if x[1]:\n",
    "            return y\n",
    "        elif y[1]:\n",
    "            return x\n",
    "        else:\n",
    "            return f(x[0], y[0]), False\n",
    "\n",
    "    combOp = op\n",
    "    seqOp = op\n",
    "\n",
    "    def aggregatePartition(iterator):\n",
    "        acc = zeroValue\n",
    "        for obj in iterator:\n",
    "            acc = seqOp(acc, obj)\n",
    "        yield acc\n",
    "\n",
    "    partiallyAggregated = rdd. \\\n",
    "        map(lambda x: (x, False)). \\\n",
    "        mapPartitions(aggregatePartition)\n",
    "\n",
    "    numPartitions = partiallyAggregated.getNumPartitions()\n",
    "\n",
    "    # binary partitions\n",
    "    scale = 2\n",
    "\n",
    "    while numPartitions > scale:\n",
    "        numPartitions /= scale\n",
    "        curNumPartitions = int(numPartitions)\n",
    "\n",
    "        def mapPartition(i, iterator):\n",
    "            for obj in iterator:\n",
    "                yield (i % curNumPartitions, obj)\n",
    "\n",
    "        partiallyAggregated = partiallyAggregated \\\n",
    "            .mapPartitionsWithIndex(mapPartition) \\\n",
    "            .reduceByKey(combOp, curNumPartitions) \\\n",
    "            .values()\n",
    "\n",
    "    # by definition it should be one partition\n",
    "    return partiallyAggregated.keys()\n",
    "\n",
    "\n",
    "\n",
    "def compute_svd(corpus_rdd, m, k, power_iters=2, extra_dims=10):\n",
    "    \"\"\"Compute SVD using GenSim Projection class. Each entry in `corpus_rdd` should a tuple array with tuples\n",
    "    of the form (token_id, value). For example, each entry could be the sparse tfidf representation of a document\n",
    "    \"\"\"\n",
    "    logging.info('Computing SVD........')\n",
    "    # Build one project per partition\n",
    "    projections_rdd = corpus_rdd. \\\n",
    "        mapPartitions(lambda x: create_projection(m, k, list(x), power_iters=power_iters, extra_dims=extra_dims))\n",
    "\n",
    "    # Merge projects one by one on the mappers\n",
    "    return binary_aggregate(projections_rdd, merge)\n",
    "\n",
    "\n",
    "def write_parquet(topic_df, topic_path):\n",
    "    logging.info('Writing parquet Files.....')\n",
    "    \n",
    "#     if not call([\"hdfs\", \"dfs\", \"-test\", \"-d\", topic_path]):\n",
    "#         logging.info('Parquet Files Exist....... Deleting Old Parquet Files')\n",
    "#         call([\"hdfs\", \"dfs\", \"-rm\", \"-r\", \"-f\", topic_path])\n",
    "\n",
    "    topic_df.write.parquet(topic_path, mode=\"overwrite\")\n",
    "    logging.info('Files Persisted to - %s', topic_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Creating Spark Session.....\n",
      "INFO:root:Spark Session Created.....\n",
      "INFO:root:Adding Libraries....\n",
      "INFO:root:Reading TFIDF Parquet Files.....\n",
      "INFO:root:Computing SVD........\n"
     ]
    }
   ],
   "source": [
    "spark = create_spark_session()\n",
    "\n",
    "import nsf_data_ingestion as nsf\n",
    "from nsf_data_ingestion.config import nsf_config\n",
    "from nsf_data_ingestion.objects import data_source_params\n",
    "# param_list = data_source_params.mapping.get(data_source_name)\n",
    "    \n",
    "# print(param_list)\n",
    "    \n",
    "    # tfidf result location\n",
    "tfidf_path = '/user/eileen/tfidf.parquet'\n",
    "    # where to save tfidf with SVD\n",
    "topic_path = 'user/eileen/topic_svd/'\n",
    "    # number of dimensions\n",
    "num_topics = 100\n",
    "#     tfidf_path = param_list.get('tfidf_path')\n",
    "    # where to save tfidf with SVD\n",
    "#     topic_path = param_list.get('tfidf_topic_path')\n",
    "    # number of dimensions\n",
    "#     num_topics = param_list.get('num_topics')\n",
    "\n",
    "logging.info('Reading TFIDF Parquet Files.....')\n",
    "tfidf_all = spark.read.parquet(tfidf_path)\n",
    "\n",
    "m = tfidf_all.first().tfidf.size\n",
    "    \n",
    "corpus_rdd = tfidf_all.\\\n",
    "                 select('tfidf').rdd.\\\n",
    "                 map(lambda row: tuple(zip(row.tfidf.indices, row.tfidf.values)))\n",
    "    \n",
    "    \n",
    "model = compute_svd(corpus_rdd, m, num_topics).first()\n",
    "    \n",
    "u = model.u\n",
    "sinv = 1 / model.s\n",
    "    \n",
    "u_bc = spark.sparkContext.broadcast(u)\n",
    "sinv_bc = spark.sparkContext.broadcast(sinv)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(tfidf):\n",
    "    return Vectors.dense((sinv_bc.value * tfidf.dot(u_bc.value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"svd path write : \",topic_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Writing parquet Files.....\n",
      "INFO:root:Files Persisted to - user/eileen/topic_svd/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "udf_transform = fn.udf(transform, VectorUDT())\n",
    "topic_df = tfidf_all.select('*', udf_transform('tfidf').alias('topic')).drop('tfidf')\n",
    "    \n",
    "topic_df.limit(4).toPandas()\n",
    "write_parquet(topic_df, topic_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
