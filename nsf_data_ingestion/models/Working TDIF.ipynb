{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import findspark\n",
    "findspark.init('/opt/cloudera/parcels/SPARK2-2.3.0.cloudera3-1.cdh5.13.3.p0.458809/lib/spark2/')\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import functions as fn\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.ml.feature import CountVectorizer, IDF, Tokenizer, RegexTokenizer, StopWordsRemover, IDF\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import Row\n",
    "\n",
    "import requests\n",
    "# getting stop words\n",
    "stop_words = requests.get('http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words').text.split()\n",
    "\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding package in /home/ananth/nsf_data_ingestion/nsf_data_ingestion/config as config\n",
      "Compiling /home/ananth/nsf_data_ingestion/nsf_data_ingestion/config/__init__.py\n",
      "Adding config/__init__.pyc\n",
      "Adding config/spark_config.pyc\n",
      "Compiling /home/ananth/nsf_data_ingestion/nsf_data_ingestion/config/data_source_params.py\n",
      "Adding config/data_source_params.pyc\n",
      "Adding config/nsf_config.pyc\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "def ziplib():\n",
    "        libpath = os.path.dirname('/home/ananth/nsf_data_ingestion/nsf_data_ingestion/config/')                  # this should point to your packages directory \\n\",\n",
    "        zippath = '/home/ananth/nsf_data_ingestion/libraries/config'+ '.zip'      # some random filename in writable directory\\n\",\n",
    "        zf = zipfile.PyZipFile(zippath, mode='w')\n",
    "        try:\n",
    "            zf.debug = 3                                              # making it verbose, good for debugging \\n\",\n",
    "            zf.writepy(libpath)\n",
    "            return zippath                                             # return path to generated zip archive\\n\",\n",
    "        finally:\n",
    "            zf.close()\n",
    "zip_path = ziplib()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session(name):\n",
    "    spark = SparkSession.builder.config(\"spark.executor.memory\", '30g')\\\n",
    "    .config('spark.executor.cores', '7')\\\n",
    "    .config('spark.cores.max', '7')\\\n",
    "    .appName('tfdf')\\\n",
    "    .getOrCreate()\n",
    "    return spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_medline(spark, processed_path):\n",
    "    \"\"\"Creates a dataframe with the columns:\n",
    "    `id`: global id\n",
    "    `source`: medline\n",
    "    `source_id`: PMID\n",
    "    `type`: publication\n",
    "    `title`\n",
    "    `venue`: journal\n",
    "    `abstract`\n",
    "    `scientists`: authors\n",
    "    `organizations`: affiliation\n",
    "    `date`: publication date\n",
    "    `content`: concatenation of abstract, affiliation, author, and journal\n",
    "    \"\"\"\n",
    "    medline_path = processed_path\n",
    "    medline_df = spark.read.parquet(medline_path)\n",
    "    return medline_df.select(\n",
    "            fn.concat(fn.lit('medline_'), fn.col('pmid')).alias('id'),\n",
    "            fn.lit('medline').alias('source'),\n",
    "            fn.col('pmid').astype('string').alias('source_id'),\n",
    "            fn.lit('publication').alias('type'),\n",
    "            'title',\n",
    "            fn.col('journal').alias('venue'),\n",
    "            'abstract',\n",
    "            fn.col('author').alias('scientists'),\n",
    "            fn.col('affiliation').alias('organizations'),\n",
    "            fn.col('pubdate').alias('date'),\n",
    "            fn.concat_ws(' ',\n",
    "                      fn.col('abstract'),\n",
    "                      fn.col('affiliation'),\n",
    "                      fn.col('author'),\n",
    "                      fn.col('journal')).alias('content'),\n",
    "            fn.lit(None).astype('string').alias('end_date'),\n",
    "            fn.lit(None).astype('string').alias('city'),\n",
    "            fn.lit(None).astype('string').alias('country'),\n",
    "            fn.lit(None).astype('string').alias('other_id')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_federal_exporter(spark, processed_path):\n",
    "    \"\"\"Creates a dataframe with the columns:\n",
    "    `id`: global id\n",
    "    `source`: federal_exporter\n",
    "    `source_id`: project id\n",
    "    `type`: grant\n",
    "    `title`: PROJECT_TITLE\n",
    "    `venue`: AGENCY\n",
    "    `abstract`\n",
    "    `scientists`: CONTACT_PI + OTHER_PIS\n",
    "    `organizations`: ORGANIZATION_NAME\n",
    "    `date`: BUDGET_START_DATE\n",
    "    `content`: concatenation of abstract, title, PIs, agency, and organization name\n",
    "    `end_date`: BUDGET_END_DATE\n",
    "    `city`: ORGANIZATION_CITY\n",
    "    `country`: ORGANIZATION_COUNTRY\n",
    "    `other_id`: PROJECT_NUMBER\n",
    "    \"\"\"\n",
    "    abstracts_df = spark.read.parquet(os.path.join(processed_path, 'abstracts.parquet'))\n",
    "    projects_df = spark.read.parquet(os.path.join(processed_path, 'projects.parquet'))\n",
    "    together_df = projects_df.join(abstracts_df, 'PROJECT_ID')\n",
    "    return together_df.select(fn.concat(fn.lit('fe_'), fn.col('PROJECT_ID')).alias('id'),\n",
    "        fn.lit('federal_exporter').alias('source'),\n",
    "        fn.col('PROJECT_ID').astype('string').alias('source_id'),\n",
    "        fn.lit('grant').alias('type'),\n",
    "        fn.col('PROJECT_TITLE').alias('title'),\n",
    "        fn.col('AGENCY').alias('venue'),\n",
    "        fn.col('ABSTRACT').alias('abstract'),\n",
    "        fn.concat_ws('; ',\n",
    "                     fn.col('CONTACT_PI_PROJECT_LEADER'),\n",
    "                     fn.col('OTHER_PIS')).alias('scientists'),\n",
    "        fn.col('ORGANIZATION_NAME').astype('string').alias('organizations'),\n",
    "        fn.col('BUDGET_START_DATE').alias('date'),\n",
    "        fn.concat_ws(' ',\n",
    "                  fn.col('ABSTRACT'),\n",
    "                  fn.col('PROJECT_TITLE'),\n",
    "                  fn.concat_ws(' ', fn.col('CONTACT_PI_PROJECT_LEADER'), fn.col('OTHER_PIS')),\n",
    "                  fn.col('AGENCY'),\n",
    "                  fn.col('ORGANIZATION_NAME')\n",
    "            ).alias('content'),\n",
    "        fn.col('BUDGET_END_DATE').alias('end_date'),\n",
    "        fn.col('ORGANIZATION_CITY').alias('city'),\n",
    "        fn.col('ORGANIZATION_COUNTRY').alias('country'),\n",
    "        fn.col('PROJECT_NUMBER').alias('other_id')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_arxiv(spark, processed_path):\n",
    "    \"\"\"Creates a dataframe with the columns:\n",
    "    `id`: global id\n",
    "    `source`: arxiv\n",
    "    `source_id`: arxiv id\n",
    "    `type`: publication\n",
    "    `title`\n",
    "    `venue`: concatenation of subjects\n",
    "    `abstract`\n",
    "    `scientists`: authors\n",
    "    `organizations`: null\n",
    "    `date`: publication date\n",
    "    `content`: concatenation of abstract, affiliation, author, and journal\n",
    "    \"\"\"\n",
    "    arxiv_path = os.path.join(processed_path)\n",
    "    arxiv_df = spark.read.parquet(arxiv_path)\n",
    "    return arxiv_df.select(\n",
    "        fn.concat(fn.lit('arxiv_'), fn.col('id')).alias('id'),\n",
    "        fn.lit('arxiv').alias('source'),\n",
    "        fn.col('id').astype('string').alias('source_id'),\n",
    "        fn.lit('publication').alias('type'),\n",
    "        'title',\n",
    "        fn.concat_ws('; ', 'subjects').alias('venue'),\n",
    "        'abstract',\n",
    "        fn.concat_ws(';', 'authors').alias('scientists'),\n",
    "        fn.lit(None).astype('string').alias('organizations'),\n",
    "        fn.col('datastamp').alias('date'),\n",
    "        fn.concat_ws(' ',\n",
    "                  fn.col('abstract'),\n",
    "                  fn.col('title'),\n",
    "                  fn.concat_ws(' ', 'authors'),\n",
    "                  fn.concat_ws(' ', 'subjects')).alias('content'),\n",
    "        fn.lit(None).astype('string').alias('end_date'),\n",
    "        fn.lit(None).astype('string').alias('city'),\n",
    "        fn.lit(None).astype('string').alias('country'),\n",
    "        fn.lit(None).astype('string').alias('other_id')\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rowid(x):\n",
    "    \"\"\"Called on a RDD when zipWithIndex() is used\"\"\"\n",
    "    return Row(row_id = x[1], **x[0].asDict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_tfidf_pipeline(content_df):\n",
    "    tokenizer = RegexTokenizer(). \\\n",
    "        setGaps(False). \\\n",
    "        setPattern('\\\\p{L}+'). \\\n",
    "        setInputCol('content'). \\\n",
    "        setOutputCol('words')\n",
    "\n",
    "    sw = StopWordsRemover() \\\n",
    "        .setStopWords(stop_words) \\\n",
    "        .setCaseSensitive(False) \\\n",
    "        .setInputCol(\"words\") \\\n",
    "        .setOutputCol(\"filtered\")\n",
    "\n",
    "    cv = CountVectorizer(). \\\n",
    "        setInputCol('filtered'). \\\n",
    "        setOutputCol('tf'). \\\n",
    "        setMinTF(1). \\\n",
    "        setMinDF(10). \\\n",
    "        setVocabSize(2 ** 17)\n",
    "\n",
    "    # fit dataframe_df\n",
    "    cv_transformer = Pipeline(stages=[tokenizer, sw, cv]).fit(content_df)\n",
    "\n",
    "    idf = IDF(minDocFreq=10). \\\n",
    "        setInputCol('tf'). \\\n",
    "        setOutputCol('tfidf')\n",
    "\n",
    "    tfidf_transformer = Pipeline(stages=[cv_transformer, idf]).fit(content_df)\n",
    "\n",
    "    return tfidf_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_path = '/user/ananth/medline/parquet/'\n",
    "fed_processed_path = '/user/ananth/data/raw/federal_exporter/'\n",
    "models_path = '/user/ananth/tdif/'\n",
    "tfidf_path = '/user/ananth/tdifupdate/'\n",
    "arxiv_path = '/user/ananth/arxiv/parquet/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_spark_session('tfdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_df = read_arxiv(spark, arxiv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+---------+-----------+--------------------+--------------------+--------------------+--------------------+-------------+----+--------------------+--------+----+-------+--------+\n",
      "|      id|source|source_id|       type|               title|               venue|            abstract|          scientists|organizations|date|             content|end_date|city|country|other_id|\n",
      "+--------+------+---------+-----------+--------------------+--------------------+--------------------+--------------------+-------------+----+--------------------+--------+----+-------+--------+\n",
      "|arxiv_\n",
      " | arxiv|       \n",
      " |publication|Electron and nucl...|physics:cond-mat;...|  A novel mathema...|Serra, Sonia Colo...|         null|  \n",
      " |  A novel mathema...|    null|null|   null|    null|\n",
      "|arxiv_\n",
      " | arxiv|       \n",
      " |publication|Uniform fractiona...|          math; stat|  The minimum abe...|Tang, Yu;Xu, Hong...|         null|  \n",
      " |  The minimum abe...|    null|null|   null|    null|\n",
      "|arxiv_\n",
      " | arxiv|       \n",
      " |publication|Bypasses for rect...|                math|  In the present ...|Dynnikov, Ivan;Pr...|         null|  \n",
      " |  In the present ...|    null|null|   null|    null|\n",
      "|arxiv_\n",
      " | arxiv|       \n",
      " |publication|Ultrathin Metalli...|physics:cond-mat;...|  There is an att...|Boström, Mathias;...|         null|  \n",
      " |  There is an att...|    null|null|   null|    null|\n",
      "|arxiv_\n",
      " | arxiv|       \n",
      " |publication|On a fractional q...|math; physics:mat...|  We use the frac...|     Carroll, Robert|         null|  \n",
      " |  We use the frac...|    null|null|   null|    null|\n",
      "|arxiv_\n",
      " | arxiv|       \n",
      " |publication|Hadronic Equation...|physics:hep-lat; ...|  The equation of...|Tawfik, Abdel Nas...|         null|  \n",
      " |  The equation of...|    null|null|   null|    null|\n",
      "|arxiv_\n",
      " | arxiv|       \n",
      " |publication|Solidification fr...|    physics:cond-mat|  We determine th...|Archer, A. J.;Rob...|         null|  \n",
      " |  We determine th...|    null|null|   null|    null|\n",
      "|arxiv_\n",
      " | arxiv|       \n",
      " |publication|Effects of Variab...|physics:astro-ph;...|  In this paper t...|Abolhasani, Ali A...|         null|  \n",
      " |  In this paper t...|    null|null|   null|    null|\n",
      "|arxiv_\n",
      " | arxiv|       \n",
      " |publication|The effect of spi...|    physics:cond-mat|  Magnetic inelas...|Heimes, Andreas;G...|         null|  \n",
      " |  Magnetic inelas...|    null|null|   null|    null|\n",
      "|arxiv_\n",
      " | arxiv|       \n",
      " |publication|A Fuzzy Approach ...|                  cs|  Recent work in ...|Boughamoura, Radh...|         null|  \n",
      " |  Recent work in ...|    null|null|   null|    null|\n",
      "|arxiv_\n",
      " | arxiv|       \n",
      " |publication|The Fate of Sub-m...|    physics:astro-ph|  We study the ra...|Jontof-Hutter, Da...|         null|  \n",
      " |  We study the ra...|    null|null|   null|    null|\n",
      "|arxiv_\n",
      " | arxiv|       \n",
      " |publication|The local Tb theo...|                math|  We prove a vers...|Hytönen, Tuomas;N...|         null|  \n",
      " |  We prove a vers...|    null|null|   null|    null|\n",
      "|arxiv_\n",
      " | arxiv|       \n",
      " |publication|Soft Computing in...|                  cs|  This paper focu...|Xing, Bo;Gao, Wen...|         null|  \n",
      " |  This paper focu...|    null|null|   null|    null|\n",
      "|arxiv_\n",
      " | arxiv|       \n",
      " |publication|Resolved stellar ...|    physics:astro-ph|  The expected im...|Greggio, L.;Falom...|         null|  \n",
      " |  The expected im...|    null|null|   null|    null|\n",
      "|arxiv_\n",
      " | arxiv|       \n",
      " |publication|Transition from s...|physics:hep-ph; p...|  We study the nu...|Watanabe, Akira;S...|         null|  \n",
      " |  We study the nu...|    null|null|   null|    null|\n",
      "|arxiv_\n",
      " | arxiv|       \n",
      " |publication|Non-null Infinite...|                  cs|  Many systems in...|Ferrucci, Luca;Ma...|         null|  \n",
      " |  Many systems in...|    null|null|   null|    null|\n",
      "|arxiv_\n",
      " | arxiv|       \n",
      " |publication|Complex deformati...|                math|  In this paper, ...|         Li, Haozhao|         null|  \n",
      " |  In this paper, ...|    null|null|   null|    null|\n",
      "|arxiv_\n",
      " | arxiv|       \n",
      " |publication|Uniform families ...|                math|  We study mean e...|    Schreiber, Marco|         null|  \n",
      " |  We study mean e...|    null|null|   null|    null|\n",
      "|arxiv_\n",
      " | arxiv|       \n",
      " |publication|Weakly-induced st...|physics:hep-ex; p...|  Weak interactio...|Gerard, J. M.;Mer...|         null|  \n",
      " |  Weak interactio...|    null|null|   null|    null|\n",
      "|arxiv_\n",
      " | arxiv|       \n",
      " |publication|Search for the PN...|physics:astro-ph;...|  In this work, t...|      Kumar, Prayush|         null|  \n",
      " |  In this work, t...|    null|null|   null|    null|\n",
      "+--------+------+---------+-----------+--------------------+--------------------+--------------------+--------------------+-------------+----+--------------------+--------+----+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "arxiv_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "medline_df = read_medline(spark, processed_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_df = read_federal_exporter(spark, fed_processed_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_list = [medline_df, fe_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_df = content_df = reduce(DataFrame.unionAll, dataframe_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = fit_tfidf_pipeline(all_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_model_path = os.path.join(models_path, 'tfidf_transformer.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer.write().overwrite().save(tfidf_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df = tfidf_transformer.transform(all_data_df). \\\n",
    "        select(all_data_df.columns + ['tfidf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df.write.parquet(tfidf_path, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
