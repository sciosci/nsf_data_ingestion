{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, findspark\n",
    "\n",
    "# temporarily changing PYSPARK_PYTHON to avoid rdd error.\n",
    "os.environ['PYSPARK_PYTHON'] = '/home/tozeng/anaconda3/bin/python'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/home/tozeng/anaconda3/bin/python'\n",
    "\n",
    "findspark.init('/opt/cloudera/parcels/SPARK2/lib/spark2/')\n",
    "\n",
    "import pyspark\n",
    "import scipy.sparse, logging, json\n",
    "import numpy as np\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import functions, SparkSession, SQLContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import udf, col, unix_timestamp, from_unixtime, coalesce, to_date\n",
    "from pyspark.ml.linalg import Vectors, _convert_to_vector, VectorUDT\n",
    "\n",
    "\n",
    "def create_session():\n",
    "    spark = SparkSession.builder.\\\n",
    "        config(\"spark.executor.instances\", '5').\\\n",
    "        config(\"spark.executor.memory\", '40g').\\\n",
    "        config('spark.executor.cores', '9').\\\n",
    "        config('spark.cores.max', '9').\\\n",
    "        config(\"spark.kryoserializer.buffer.max.mb\", \"2000\").\\\n",
    "        config('spark.jars', '/home/eileen/nsf_data_ingestion/libraries/elasticsearch-hadoop-7.10.1.jar').\\\n",
    "        appName('kimun_loader').\\\n",
    "        getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse(tup):\n",
    "    d = {}\n",
    "    d['id']=tup['id']\n",
    "    d['city']=tup['city']\n",
    "    d['country'] = tup['country']\n",
    "    d['date'] = tup['date']\n",
    "    d['documentType'] = tup['type']\n",
    "    d['endDate'] = tup['end_date']\n",
    "    d['organizations'] = tup['organizations']\n",
    "    d['otherID'] = tup['other_id']\n",
    "    d['scientists'] = tup['scientists']\n",
    "    d['sourceID'] = tup['source_id']\n",
    "    d['source'] = tup['source']\n",
    "    d['summary'] = tup['abstract']\n",
    "    d['text'] = tup['content']\n",
    "    d['title'] = tup['title']\n",
    "    d['venue']=tup['venue']\n",
    "    d['topicNorm'] = list(tup['topic'])\n",
    "    return (d['id'], json.dumps(d))\n",
    "\n",
    "\n",
    "def dense_to_sparse(vector):\n",
    "    sparse = _convert_to_vector(scipy.sparse.csc_matrix(vector.toArray()).T)\n",
    "    #matrix = np.array(sparse.toArray()).as_matrix().reshape(-1,1)\n",
    "    return sparse\n",
    "\n",
    "##### elastic push function update pending\n",
    "# def elastic_push(result):\n",
    "#     es_write_conf = {\n",
    "#             \"es.nodes\" : \"128.230.247.186\",\n",
    "#             \"es.port\" : \"9201\",\n",
    "#             \"es.resource\" : 'kimun/documents',\n",
    "#             \"es.input.json\": \"yes\",\n",
    "#             \"es.mapping.id\": \"id\",\n",
    "#             \"es.batch.size.entries\": \"5000\",\n",
    "#             \"es.batch.write.retry.wait\": \"3000\"\n",
    "#         }\n",
    "\n",
    "#     result.saveAsNewAPIHadoopFile(\n",
    "#             path='-',\n",
    "#             outputFormatClass=\"org.elasticsearch.hadoop.mr.EsOutputFormat\", keyClass=\"org.apache.hadoop.io.NullWritable\",\n",
    "#             valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\",\n",
    "#             conf=es_write_conf)\n",
    "\n",
    "##### function to be updated\n",
    "# def kimun_load():\n",
    "#     spark = create_session()\n",
    "#     sqlContext = SQLContext(spark.sparkContext)\n",
    "#     topic_df = sqlContext.read.parquet('/user/sghosh08/tfidf_topic/')\n",
    "#     topic_rdd = topic_df.rdd\n",
    "#     result = topic_rdd.map(parse)\n",
    "#     elastic_push(result)\n",
    "\n",
    "\n",
    "def to_date_(col, formats=(\"MM/dd/yyyy\", \"yyyy\")):\n",
    "    # Spark 2.2 or later syntax, for < 2.2 use unix_timestamp and cast\n",
    "    return coalesce(*[to_date(col, f) for f in formats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"running kimun_load\")\n",
    "spark = create_session()\n",
    "sqlContext = SQLContext(spark.sparkContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_df = sqlContext.read.parquet('/user/eileen/topic_svd/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new date formatting\n",
    "topic_df.date = topic_df.select('date', from_unixtime(unix_timestamp('date', 'yyy')).alias('date'))\n",
    "split_col = pyspark.sql.functions.split(topic_df['date'], '-')\n",
    "topic_df = topic_df.withColumn('date', split_col.getItem(0))\n",
    "topic_df = topic_df.withColumn(\"formatted_date\", to_date_(\"date\"))\n",
    "topic_df.formatted_date = topic_df.select('formatted_date',from_unixtime(unix_timestamp('formatted_date','yyy')).alias('formatted_date'))\n",
    "split_col = pyspark.sql.functions.split(topic_df['formatted_date'], '-')\n",
    "topic_df = topic_df.withColumn('formatted_date', split_col.getItem(0))\n",
    "topic_df.date = topic_df.select(topic_df.formatted_date).alias('date')\n",
    "columns = topic_df.columns\n",
    "topic_df = topic_df.drop('date')\n",
    "topic_df = topic_df.withColumnRenamed('formatted_date', 'date')\n",
    "topic_df = topic_df.withColumn(\"title\", F.regexp_replace(F.regexp_replace(F.regexp_replace(\"title\", \"\\\\]\\\\[\", \"\"), \"\\\\[\",\"\"),\"\\\\]\",\"\"))\n",
    "topic_df = topic_df.withColumn(\"abstract\", F.regexp_replace(F.regexp_replace(F.regexp_replace(\"abstract\", \"\\\\]\\\\[\", \"\"), \"\\\\[\", \"\"), \"\\\\]\", \"\"))\n",
    "columns = topic_df.columns\n",
    "val2 = topic_df.select(columns).groupBy(['title', 'scientists', 'venue']).agg(F.min('date'))\n",
    "val2 = val2.withColumnRenamed('min(date)', 'date')\n",
    "val3 = val2.join(topic_df, ['title', 'scientists', 'venue', 'date'])\n",
    "val3 = val3.withColumn(\"date\", val3[\"date\"].cast(\"int\"))\n",
    "val3 = val3.withColumn(\"date\", val3[\"date\"].cast(\"string\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before conf \n"
     ]
    }
   ],
   "source": [
    "\n",
    "#date formatting end\n",
    "sc = spark.sparkContext\n",
    "print(\"before conf \")\n",
    "es_write_conf = {\n",
    "        \"es.nodes\" : \"128.230.247.186\",\\\n",
    "        \"es.port\" : \"9201\",\\\n",
    "        # TODO: check if the name of index is consistent with the current one\n",
    "        \"es.resource\" : 'kimun/documents',\\\n",
    "        \"es.input.json\": \"yes\",\\\n",
    "        \"es.mapping.id\": \"id\",\\\n",
    "        \"es.batch.size.entries\": \"5000\",\\\n",
    "        \"es.batch.write.retry.wait\": \"3000\"\n",
    "    }\n",
    "\n",
    "rdd = sc.newAPIHadoopRDD(\"org.elasticsearch.hadoop.mr.EsInputFormat\", \"org.apache.hadoop.io.NullWritable\", \"org.elasticsearch.hadoop.mr.LinkedMapWritable\", conf=es_write_conf)\n",
    "topic_rdd = val3.rdd\n",
    "result = topic_rdd.map(parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.repartition(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-8c6840cdbc7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mkeyClass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"org.apache.hadoop.io.NullWritable\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvalueClass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"org.elasticsearch.hadoop.mr.LinkedMapWritable\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     conf=es_write_conf)\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Data Loaded in Elastic Search check kibana index count\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/rdd.py\u001b[0m in \u001b[0;36msaveAsNewAPIHadoopFile\u001b[0;34m(self, path, outputFormatClass, keyClass, valueClass, keyConverter, valueConverter, conf)\u001b[0m\n\u001b[1;32m   1436\u001b[0m                                                        \u001b[0moutputFormatClass\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1437\u001b[0m                                                        \u001b[0mkeyClass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalueClass\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1438\u001b[0;31m                                                        keyConverter, valueConverter, jconf)\n\u001b[0m\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msaveAsHadoopDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeyConverter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalueConverter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hzhuang/anaconda3/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result.saveAsNewAPIHadoopFile(\n",
    "    path='-',\n",
    "    outputFormatClass=\"org.elasticsearch.hadoop.mr.EsOutputFormat\",\n",
    "    keyClass=\"org.apache.hadoop.io.NullWritable\",\n",
    "    valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\",\n",
    "    conf=es_write_conf)\n",
    "\n",
    "logging.info(\"Data Loaded in Elastic Search check kibana index count\")\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
