{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import os, findspark\n",
    "\n",
    "# temporarily changing PYSPARK_PYTHON to avoid rdd error.\n",
    "os.environ['PYSPARK_PYTHON'] = '/home/tozeng/anaconda3/bin/python'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/home/tozeng/anaconda3/bin/python'\n",
    "\n",
    "findspark.init('/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/')\n",
    "#findspark.init('/opt/cloudera/parcels/SPARK2-2.3.0.cloudera3-1.cdh5.13.3.p0.458809/lib/spark2/')\n",
    "import pyspark\n",
    "import scipy.sparse, logging, json\n",
    "import numpy as np\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import functions, SparkSession, SQLContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import udf, col, unix_timestamp, from_unixtime, coalesce, to_date\n",
    "from pyspark.ml.linalg import Vectors, _convert_to_vector, VectorUDT\n",
    "\n",
    "\n",
    "def create_session():\n",
    "    spark = SparkSession.builder.\\\n",
    "        config(\"spark.executor.instances\", '5').\\\n",
    "        config(\"spark.executor.memory\", '40g').\\\n",
    "        config('spark.executor.cores', '9').\\\n",
    "        config('spark.cores.max', '9').\\\n",
    "        config(\"spark.kryoserializer.buffer.max.mb\", \"2000\").\\\n",
    "        config('spark.jars', '/home/eileen/nsf_data_ingestion/libraries/elasticsearch-hadoop-7.10.1.jar').\\\n",
    "        appName('kimun_loader').\\\n",
    "        getOrCreate()\n",
    "    return spark\n",
    "\n",
    "def parse(tup):\n",
    "    d = {}\n",
    "    d['id']=tup['id']\n",
    "    d['city']=tup['city']\n",
    "    d['country'] = tup['country']\n",
    "    d['date'] = tup['date']\n",
    "    d['documentType'] = tup['type']\n",
    "    d['endDate'] = tup['end_date']\n",
    "    d['organizations'] = tup['organizations']\n",
    "    d['otherID'] = tup['other_id']\n",
    "    d['scientists'] = tup['scientists']\n",
    "    d['sourceID'] = tup['source_id']\n",
    "    d['source'] = tup['source']\n",
    "    d['summary'] = tup['abstract']\n",
    "    d['text'] = tup['content']\n",
    "    d['title'] = tup['title']\n",
    "    d['venue']=tup['venue']\n",
    "    d['topicNorm'] = list(tup['topic'])\n",
    "    return (d['id'], json.dumps(d))\n",
    "\n",
    "\n",
    "def dense_to_sparse(vector):\n",
    "    sparse = _convert_to_vector(scipy.sparse.csc_matrix(vector.toArray()).T)\n",
    "    #matrix = np.array(sparse.toArray()).as_matrix().reshape(-1,1)\n",
    "    return sparse\n",
    "\n",
    "##### elastic push function update pending\n",
    "# def elastic_push(result):\n",
    "#     es_write_conf = {\n",
    "#             \"es.nodes\" : \"128.230.247.186\",\n",
    "#             \"es.port\" : \"9201\",\n",
    "#             \"es.resource\" : 'kimun/documents',\n",
    "#             \"es.input.json\": \"yes\",\n",
    "#             \"es.mapping.id\": \"id\",\n",
    "#             \"es.batch.size.entries\": \"5000\",\n",
    "#             \"es.batch.write.retry.wait\": \"3000\"\n",
    "#         }\n",
    "\n",
    "#     result.saveAsNewAPIHadoopFile(\n",
    "#             path='-',\n",
    "#             outputFormatClass=\"org.elasticsearch.hadoop.mr.EsOutputFormat\", keyClass=\"org.apache.hadoop.io.NullWritable\",\n",
    "#             valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\",\n",
    "#             conf=es_write_conf)\n",
    "\n",
    "##### function to be updated\n",
    "# def kimun_load():\n",
    "#     spark = create_session()\n",
    "#     sqlContext = SQLContext(spark.sparkContext)\n",
    "#     topic_df = sqlContext.read.parquet('/user/sghosh08/tfidf_topic/')\n",
    "#     topic_rdd = topic_df.rdd\n",
    "#     result = topic_rdd.map(parse)\n",
    "#     elastic_push(result)\n",
    "\n",
    "\n",
    "def to_date_(col, formats=(\"MM/dd/yyyy\", \"yyyy\")):\n",
    "    # Spark 2.2 or later syntax, for < 2.2 use unix_timestamp and cast\n",
    "    return coalesce(*[to_date(col, f) for f in formats])\n",
    "\n",
    "def kimun_load():\n",
    "    logging.info(\"running kimun_load\")\n",
    "    spark = create_session()\n",
    "    sqlContext = SQLContext(spark.sparkContext)\n",
    "\n",
    "    #topic_df = sqlContext.read.parquet('/user/hzhan212/eileen/svd.sample') # for test purpose\n",
    "    topic_df = sqlContext.read.parquet('/user/eileen/topic_svd/')\n",
    "\n",
    "    #new date formatting\n",
    "    topic_df.date = topic_df.select('date', from_unixtime(unix_timestamp('date', 'yyy')).alias('date'))\n",
    "    split_col = pyspark.sql.functions.split(topic_df['date'], '-')\n",
    "    topic_df = topic_df.withColumn('date', split_col.getItem(0))\n",
    "    topic_df = topic_df.withColumn(\"formatted_date\", to_date_(\"date\"))\n",
    "    topic_df.formatted_date = topic_df.select('formatted_date',from_unixtime(unix_timestamp('formatted_date','yyy')).alias('formatted_date'))\n",
    "    split_col = pyspark.sql.functions.split(topic_df['formatted_date'], '-')\n",
    "    topic_df = topic_df.withColumn('formatted_date', split_col.getItem(0))\n",
    "    topic_df.date = topic_df.select(topic_df.formatted_date).alias('date')\n",
    "    columns = topic_df.columns\n",
    "    topic_df = topic_df.drop('date')\n",
    "    topic_df = topic_df.withColumnRenamed('formatted_date', 'date')\n",
    "    topic_df = topic_df.withColumn(\"title\", F.regexp_replace(F.regexp_replace(F.regexp_replace(\"title\", \"\\\\]\\\\[\", \"\"), \"\\\\[\",\"\"),\"\\\\]\",\"\"))\n",
    "    topic_df = topic_df.withColumn(\"abstract\", F.regexp_replace(F.regexp_replace(F.regexp_replace(\"abstract\", \"\\\\]\\\\[\", \"\"), \"\\\\[\", \"\"), \"\\\\]\", \"\"))\n",
    "    columns = topic_df.columns\n",
    "    val2 = topic_df.select(columns).groupBy(['title', 'scientists', 'venue']).agg(F.min('date'))\n",
    "    val2 = val2.withColumnRenamed('min(date)', 'date')\n",
    "    val3 = val2.join(topic_df, ['title', 'scientists', 'venue', 'date'])\n",
    "    val3 = val3.withColumn(\"date\", val3[\"date\"].cast(\"int\"))\n",
    "    val3 = val3.withColumn(\"date\", val3[\"date\"].cast(\"string\"))\n",
    "    #date formatting end\n",
    "    sc = spark.sparkContext\n",
    "    print(\"before conf \")\n",
    "    es_write_conf = {\n",
    "            \"es.nodes\" : \"128.230.247.186\",\\\n",
    "            \"es.port\" : \"9201\",\\\n",
    "            # TODO: check if the name of index is consistent with the current one\n",
    "            \"es.resource\" : 'kimun/documents',\\\n",
    "            \"es.input.json\": \"yes\",\\\n",
    "            \"es.mapping.id\": \"id\",\\\n",
    "            \"es.batch.size.entries\": \"5000\",\\\n",
    "            \"es.batch.write.retry.wait\": \"3000\"\n",
    "        }\n",
    "\n",
    "    rdd = sc.newAPIHadoopRDD(\"org.elasticsearch.hadoop.mr.EsInputFormat\", \"org.apache.hadoop.io.NullWritable\", \"org.elasticsearch.hadoop.mr.LinkedMapWritable\", conf=es_write_conf)\n",
    "    topic_rdd = val3.rdd\n",
    "    result = topic_rdd.map(parse)\n",
    "    result = result.repartition(1)\n",
    "    print(\"after repartition\")\n",
    "\n",
    "    result.saveAsNewAPIHadoopFile(\n",
    "        path='-',\n",
    "        outputFormatClass=\"org.elasticsearch.hadoop.mr.EsOutputFormat\",\n",
    "        keyClass=\"org.apache.hadoop.io.NullWritable\",\n",
    "        valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\",\n",
    "        conf=es_write_conf)\n",
    "\n",
    "    logging.info(\"Data Loaded in Elastic Search check kibana index count\")\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before conf \n",
      "after repartition\n"
     ]
    }
   ],
   "source": [
    "kimun_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\r\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      "100  1300  100  1300    0     0  50053      0 --:--:-- --:--:-- --:--:-- 52000\r\n"
     ]
    }
   ],
   "source": [
    "!curl -XGET \"http://128.230.247.186:9201/_cat/indices\" | grep kimun_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
