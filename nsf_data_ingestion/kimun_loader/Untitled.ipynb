{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "findspark.init('/opt/cloudera/parcels/SPARK2-2.3.0.cloudera3-1.cdh5.13.3.p0.458809/lib/spark2/')\n",
    "import pyspark\n",
    "from pyspark.sql import functions\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "import scipy.sparse\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "from pyspark.sql.functions import from_unixtime\n",
    "from pyspark.sql.functions import coalesce, to_date\n",
    "from pyspark.ml.linalg import Vectors, _convert_to_vector, VectorUDT\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import coalesce, to_date\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def create_session():\n",
    "    spark = SparkSession.builder.config(\"spark.executor.instances\", '5')\\\n",
    "        .config(\"spark.executor.memory\", '40g')\\\n",
    "        .config('spark.executor.cores', '9')\\\n",
    "        .config('spark.cores.max', '9')\\\n",
    "        .config('spark.jars', '/home/sghosh08/nsf_new/nsf_data_ingestion/libraries/elasticsearch-hadoop-6.7.1.jar')\\\n",
    "        .appName('kimun_loader')\\\n",
    "        .getOrCreate()\n",
    "    return spark\n",
    "    \n",
    "def parse(tup):\n",
    "    d = {}\n",
    "    d['id']=tup['id']\n",
    "    d['city']=tup['city']\n",
    "    d['country'] = tup['country']\n",
    "    d['date'] = tup['date']\n",
    "    d['documentType'] = tup['type']\n",
    "    d['endDate'] = tup['end_date']\n",
    "    d['organizations'] = tup['organizations']\n",
    "    d['otherID'] = tup['other_id']\n",
    "    d['scientists'] = tup['scientists']\n",
    "    d['sourceID'] = tup['source_id']\n",
    "    d['summary'] = tup['abstract']\n",
    "    d['text'] = tup['content']\n",
    "    d['title'] = tup['title']\n",
    "    d['venue']=tup['venue']\n",
    "    d['topicNorm'] = list(tup['topic'])\n",
    "    return (d['id'], json.dumps(d))\n",
    "\n",
    "def dense_to_sparse(vector):\n",
    "    sparse = _convert_to_vector(scipy.sparse.csc_matrix(vector.toArray()).T)\n",
    "    #matrix = np.array(sparse.toArray()).as_matrix().reshape(-1,1)\n",
    "    return sparse\n",
    "\n",
    "def to_date_(col, formats=(\"MM/dd/yyyy\", \"yyyy\")):\n",
    "    # Spark 2.2 or later syntax, for < 2.2 use unix_timestamp and cast\n",
    "    return coalesce(*[to_date(col, f) for f in formats])\n",
    "\n",
    "def elastic_push(result):\n",
    "    es_write_conf = {\n",
    "            \"es.nodes\" : \"128.230.247.186\",\n",
    "            \"es.port\" : \"9201\",\n",
    "            \"es.resource\" : 'kimun_version5/documents',\n",
    "            \"es.input.json\": \"yes\",\n",
    "            \"es.mapping.id\": \"id\",\n",
    "            \"es.batch.size.entries\": \"5000\",\n",
    "            \"es.batch.write.retry.wait\": \"3000\"\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_session()\n",
    "sqlContext = SQLContext(spark.sparkContext)\n",
    "topic_df = sqlContext.read.parquet('/user/eileen/user/eileen/topic_svd/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_df.date = topic_df.select('date', from_unixtime(unix_timestamp('date', 'yyy')).alias('date'))\n",
    "split_col = pyspark.sql.functions.split(topic_df['date'], '-')\n",
    "topic_df = topic_df.withColumn('date', split_col.getItem(0))\n",
    "topic_df = topic_df.withColumn(\"formatted_date\", to_date_(\"date\"))\n",
    "topic_df.formatted_date = \\\n",
    "            topic_df.select('formatted_date',from_unixtime(unix_timestamp('formatted_date','yyy')).alias('formatted_date'))\n",
    "split_col = pyspark.sql.functions.split(topic_df['formatted_date'], '-')\n",
    "topic_df = topic_df.withColumn('formatted_date', split_col.getItem(0))\n",
    "topic_df.date = topic_df.select(topic_df.formatted_date).alias('date')\n",
    "columns = topic_df.columns\n",
    "topic_df = topic_df.drop('date')\n",
    "topic_df = topic_df.withColumnRenamed('formatted_date', 'date')\n",
    "topic_df = topic_df \\\n",
    "                .withColumn(\"title\", F.regexp_replace(F.regexp_replace(F.regexp_replace(\"title\", \"\\\\]\\\\[\", \"\"), \"\\\\[\",\"\"),\"\\\\]\",\"\"))\n",
    "topic_df = topic_df.withColumn(\"abstract\", F.regexp_replace(F.regexp_replace(F.regexp_replace \\\n",
    "                                                                             (\"abstract\", \"\\\\]\\\\[\", \"\"), \"\\\\[\", \"\"), \"\\\\]\", \"\"))\n",
    "                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before conf \n"
     ]
    }
   ],
   "source": [
    "columns = topic_df.columns\n",
    "val2 = topic_df.select(columns).groupBy(['title', 'scientists', 'venue']).agg(F.min('date'))\n",
    "val2 = val2.withColumnRenamed('min(date)', 'date')\n",
    "val3 = val2.join(topic_df, ['title', 'scientists', 'venue', 'date'])\n",
    "val3 = val3.withColumn(\"date\", val3[\"date\"].cast(\"int\"))\n",
    "val3 = val3.withColumn(\"date\", val3[\"date\"].cast(\"string\"))\n",
    "    #date formatting end\n",
    "sc = spark.sparkContext\n",
    "print(\"before conf \")\n",
    "es_write_conf = {\n",
    "            \"es.nodes\" : \"128.230.247.186\",\\\n",
    "            \"es.port\" : \"9201\",\\\n",
    "            \"es.resource\" : 'kimun_version5/documents',\\\n",
    "            \"es.input.json\": \"yes\",\\\n",
    "            \"es.mapping.id\": \"id\",\\\n",
    "            \"es.batch.size.entries\": \"5000\",\\\n",
    "            \"es.batch.write.retry.wait\": \"3000\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before push\n",
      "after repartition\n"
     ]
    }
   ],
   "source": [
    "print(\"before push\")\n",
    "rdd = sc.newAPIHadoopRDD(\"org.elasticsearch.hadoop.mr.EsInputFormat\", \"org.apache.hadoop.io.NullWritable\", \"org.elasticsearch.hadoop.mr.LinkedMapWritable\", conf=es_write_conf)\n",
    "topic_rdd = val3.rdd\n",
    "result = topic_rdd.map(parse)\n",
    "result = result.repartition(1)\n",
    "print(\"after repartition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-399e9f784b6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0moutputFormatClass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"org.elasticsearch.hadoop.mr.EsOutputFormat\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeyClass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"org.apache.hadoop.io.NullWritable\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mvalueClass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"org.elasticsearch.hadoop.mr.LinkedMapWritable\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         conf=es_write_conf)\n\u001b[0m",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2-2.3.0.cloudera3-1.cdh5.13.3.p0.458809/lib/spark2/python/pyspark/rdd.py\u001b[0m in \u001b[0;36msaveAsNewAPIHadoopFile\u001b[0;34m(self, path, outputFormatClass, keyClass, valueClass, keyConverter, valueConverter, conf)\u001b[0m\n\u001b[1;32m   1451\u001b[0m                                                        \u001b[0moutputFormatClass\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1452\u001b[0m                                                        \u001b[0mkeyClass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalueClass\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1453\u001b[0;31m                                                        keyConverter, valueConverter, jconf)\n\u001b[0m\u001b[1;32m   1454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1455\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msaveAsHadoopDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeyConverter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalueConverter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2-2.3.0.cloudera3-1.cdh5.13.3.p0.458809/lib/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2-2.3.0.cloudera3-1.cdh5.13.3.p0.458809/lib/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2-2.3.0.cloudera3-1.cdh5.13.3.p0.458809/lib/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tozeng/anaconda3/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result.saveAsNewAPIHadoopFile(\\\n",
    "        path='-',\\\n",
    "        outputFormatClass=\"org.elasticsearch.hadoop.mr.EsOutputFormat\", keyClass=\"org.apache.hadoop.io.NullWritable\",\\\n",
    "        valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\",\\\n",
    "        conf=es_write_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "environ{'USER': 'eileen',\n",
       "        'JUPYTERHUB_HOST': '',\n",
       "        'JUPYTERHUB_USER': 'eileen',\n",
       "        'HOME': '/home/eileen',\n",
       "        'JUPYTERHUB_OAUTH_CALLBACK_URL': '/user/eileen/oauth_callback',\n",
       "        'JUPYTERHUB_API_URL': 'http://127.0.0.1:8081/hub/api',\n",
       "        'JUPYTERHUB_CLIENT_ID': 'jupyterhub-user-eileen',\n",
       "        'PATH': '/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin',\n",
       "        'JUPYTERHUB_ACTIVITY_URL': 'http://127.0.0.1:8081/hub/api/users/eileen/activity',\n",
       "        'LANG': 'en_US.UTF-8',\n",
       "        'SHELL': '/bin/bash',\n",
       "        'JUPYTERHUB_ADMIN_ACCESS': '1',\n",
       "        'JUPYTERHUB_SERVICE_PREFIX': '/user/eileen/',\n",
       "        'JAVA_HOME': '/usr/java/jdk1.8.0_162',\n",
       "        'JUPYTERHUB_API_TOKEN': '53df9a58f5334e50b4a9e839457bc9ab',\n",
       "        'PWD': '/home/eileen',\n",
       "        'JUPYTERHUB_SERVER_NAME': '',\n",
       "        'JUPYTERHUB_BASE_URL': '/',\n",
       "        'JPY_API_TOKEN': '53df9a58f5334e50b4a9e839457bc9ab',\n",
       "        'KERNEL_LAUNCH_TIMEOUT': '40',\n",
       "        'JPY_PARENT_PID': '29579',\n",
       "        'TERM': 'xterm-color',\n",
       "        'CLICOLOR': '1',\n",
       "        'PAGER': 'cat',\n",
       "        'GIT_PAGER': 'cat',\n",
       "        'MPLBACKEND': 'module://ipykernel.pylab.backend_inline'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
