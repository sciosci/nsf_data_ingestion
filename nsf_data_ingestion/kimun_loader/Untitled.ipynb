{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "findspark.init('/opt/cloudera/parcels/SPARK2-2.3.0.cloudera3-1.cdh5.13.3.p0.458809/lib/spark2/')\n",
    "import pyspark\n",
    "from pyspark.sql import functions\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "import scipy.sparse\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "from pyspark.sql.functions import from_unixtime\n",
    "from pyspark.sql.functions import coalesce, to_date\n",
    "from pyspark.ml.linalg import Vectors, _convert_to_vector, VectorUDT\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import coalesce, to_date\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def create_session():\n",
    "    spark = SparkSession.builder.config(\"spark.executor.instances\", '5')\\\n",
    "        .config(\"spark.executor.memory\", '40g')\\\n",
    "        .config('spark.executor.cores', '9')\\\n",
    "        .config('spark.cores.max', '9')\\\n",
    "        .config('spark.jars', '/home/sghosh08/nsf_new/nsf_data_ingestion/libraries/elasticsearch-hadoop-6.7.1.jar')\\\n",
    "        .appName('kimun_loader')\\\n",
    "        .getOrCreate()\n",
    "    return spark\n",
    "    \n",
    "def parse(tup):\n",
    "    d = {}\n",
    "    d['id']=tup['id']\n",
    "    d['city']=tup['city']\n",
    "    d['country'] = tup['country']\n",
    "    d['date'] = tup['date']\n",
    "    d['documentType'] = tup['type']\n",
    "    d['endDate'] = tup['end_date']\n",
    "    d['organizations'] = tup['organizations']\n",
    "    d['otherID'] = tup['other_id']\n",
    "    d['scientists'] = tup['scientists']\n",
    "    d['sourceID'] = tup['source_id']\n",
    "    d['summary'] = tup['abstract']\n",
    "    d['text'] = tup['content']\n",
    "    d['title'] = tup['title']\n",
    "    d['venue']=tup['venue']\n",
    "    d['topicNorm'] = list(tup['topic'])\n",
    "    return (d['id'], json.dumps(d))\n",
    "\n",
    "def dense_to_sparse(vector):\n",
    "    sparse = _convert_to_vector(scipy.sparse.csc_matrix(vector.toArray()).T)\n",
    "    #matrix = np.array(sparse.toArray()).as_matrix().reshape(-1,1)\n",
    "    return sparse\n",
    "\n",
    "def to_date_(col, formats=(\"MM/dd/yyyy\", \"yyyy\")):\n",
    "    # Spark 2.2 or later syntax, for < 2.2 use unix_timestamp and cast\n",
    "    return coalesce(*[to_date(col, f) for f in formats])\n",
    "\n",
    "def elastic_push(result):\n",
    "    es_write_conf = {\n",
    "            \"es.nodes\" : \"128.230.247.186\",\n",
    "            \"es.port\" : \"9201\",\n",
    "            \"es.resource\" : 'kimun_version4/documents',\n",
    "            \"es.input.json\": \"yes\",\n",
    "            \"es.mapping.id\": \"id\",\n",
    "            \"es.batch.size.entries\": \"5000\",\n",
    "            \"es.batch.write.retry.wait\": \"3000\"\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_session()\n",
    "sqlContext = SQLContext(spark.sparkContext)\n",
    "topic_df = sqlContext.read.parquet('/user/eileen/user/eileen/topic_svd/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_df.date = topic_df.select('date', from_unixtime(unix_timestamp('date', 'yyy')).alias('date'))\n",
    "split_col = pyspark.sql.functions.split(topic_df['date'], '-')\n",
    "topic_df = topic_df.withColumn('date', split_col.getItem(0))\n",
    "topic_df = topic_df.withColumn(\"formatted_date\", to_date_(\"date\"))\n",
    "topic_df.formatted_date = \\\n",
    "            topic_df.select('formatted_date',from_unixtime(unix_timestamp('formatted_date','yyy')).alias('formatted_date'))\n",
    "split_col = pyspark.sql.functions.split(topic_df['formatted_date'], '-')\n",
    "topic_df = topic_df.withColumn('formatted_date', split_col.getItem(0))\n",
    "topic_df.date = topic_df.select(topic_df.formatted_date).alias('date')\n",
    "columns = topic_df.columns\n",
    "topic_df = topic_df.drop('date')\n",
    "topic_df = topic_df.withColumnRenamed('formatted_date', 'date')\n",
    "topic_df = topic_df \\\n",
    "                .withColumn(\"title\", F.regexp_replace(F.regexp_replace(F.regexp_replace(\"title\", \"\\\\]\\\\[\", \"\"), \"\\\\[\",\"\"),\"\\\\]\",\"\"))\n",
    "topic_df = topic_df.withColumn(\"abstract\", F.regexp_replace(F.regexp_replace(F.regexp_replace \\\n",
    "                                                                             (\"abstract\", \"\\\\]\\\\[\", \"\"), \"\\\\[\", \"\"), \"\\\\]\", \"\"))\n",
    "                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before conf \n"
     ]
    }
   ],
   "source": [
    "columns = topic_df.columns\n",
    "val2 = topic_df.select(columns).groupBy(['title', 'scientists', 'venue']).agg(F.min('date'))\n",
    "val2 = val2.withColumnRenamed('min(date)', 'date')\n",
    "val3 = val2.join(topic_df, ['title', 'scientists', 'venue', 'date'])\n",
    "val3 = val3.withColumn(\"date\", val3[\"date\"].cast(\"int\"))\n",
    "val3 = val3.withColumn(\"date\", val3[\"date\"].cast(\"string\"))\n",
    "    #date formatting end\n",
    "sc = spark.sparkContext\n",
    "print(\"before conf \")\n",
    "es_write_conf = {\n",
    "            \"es.nodes\" : \"128.230.247.186\",\\\n",
    "            \"es.port\" : \"9201\",\\\n",
    "            \"es.resource\" : 'kimun_version5/documents',\\\n",
    "            \"es.input.json\": \"yes\",\\\n",
    "            \"es.mapping.id\": \"id\",\\\n",
    "            \"es.batch.size.entries\": \"5000\",\\\n",
    "            \"es.batch.write.retry.wait\": \"3000\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before push\n",
      "after repartition\n"
     ]
    }
   ],
   "source": [
    "print(\"before push\")\n",
    "rdd = sc.newAPIHadoopRDD(\"org.elasticsearch.hadoop.mr.EsInputFormat\", \"org.apache.hadoop.io.NullWritable\", \"org.elasticsearch.hadoop.mr.LinkedMapWritable\", conf=es_write_conf)\n",
    "topic_rdd = val3.rdd\n",
    "result = topic_rdd.map(parse)\n",
    "result = result.repartition(1)\n",
    "print(\"after repartition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.saveAsNewAPIHadoopFile(\\\n",
    "        path='-',\\\n",
    "        outputFormatClass=\"org.elasticsearch.hadoop.mr.EsOutputFormat\", keyClass=\"org.apache.hadoop.io.NullWritable\",\\\n",
    "        valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\",\\\n",
    "        conf=es_write_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
