{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/')\n",
    "import os, findspark\n",
    "os.environ['PYSPARK_PYTHON'] = '/home/tozeng/anaconda3/bin/python'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/home/tozeng/anaconda3/bin/python'\n",
    "import sys\n",
    "sys.path.append('/home/eileen/nsf_data_ingestion/')\n",
    "import zipfile\n",
    "import zipimport\n",
    "import io\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "from pyspark.sql import functions\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import rank, max, sum, desc\n",
    "import zlib\n",
    "import importlib\n",
    "import os\n",
    "from os import path\n",
    "from shutil import copyfile\n",
    "from shutil import rmtree\n",
    "from subprocess import call\n",
    "\n",
    "#sys.path.append('/home/ananth/nsf_data_ingestion/')\n",
    "\n",
    "from nsf_data_ingestion.config import spark_config\n",
    "from nsf_data_ingestion.objects import data_source_params\n",
    "\n",
    "\n",
    "def create_session(libraries_list):\n",
    "    logging.info('Creating Spark Session....')\n",
    "    spark = SparkSession.builder.config(\"spark.executor.instances\", spark_config.exec_instance).\\\n",
    "                                 config(\"spark.executor.memory\", spark_config.exec_mem).\\\n",
    "                                 config('spark.executor.cores', spark_config.exec_cores).\\\n",
    "                                 config('spark.cores.max', spark_config.exec_max_cores).\\\n",
    "                                 appName(data_source_name).getOrCreate()\n",
    "    spark.sparkContext.addPyFile('/home/eileen/nsf_data_ingestion/libraries/pubmed_parser-0.1.0-py3.6.egg')\n",
    "    spark.sparkContext.addPyFile('/home/eileen/nsf_data_ingestion/libraries/Unidecode-1.1.1-py3.6.egg')\n",
    "#         logging.info('Adding Libraries' + str(library))\n",
    "#         spark.sparkContext.addPyFile(library)    # adding libraries\n",
    "#         spark.sparkContext.addPyFile(library)\n",
    "\n",
    "        \n",
    "    return spark\n",
    "\n",
    "def parse_gzip_medline_str(filepath_list):\n",
    "    import pubmed_parser as pp\n",
    "    import pyarrow as pa\n",
    "    import gzip\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    data_source_name = 'medline'\n",
    "    params_list = data_source_params.mapping.get(data_source_name)\n",
    "    #medline_xml_path = '/user/eileen/medline/medline_data/'\n",
    "    medline_xml_path= params_list.get('xml_path')\n",
    "    print(medline_xml_path)\n",
    "    #medline_parquet_path = '/user/eileen/medline/parquet/'\n",
    "    import pubmed_parser as pp\n",
    "    medline_parquet_path= params_list.get('parquet_path')\n",
    "    libraries_list = spark_config.libraries_list\n",
    "    \n",
    "    print(\"Reading from {} and writing to {}.\".format(medline_xml_path, medline_parquet_path))\n",
    "    \n",
    "    spark = create_session(libraries_list)\n",
    "    hdfs_data = spark.sparkContext.wholeTextFiles(os.path.join(medline_xml_path, '*.xml.gz'), minPartitions=10000)\n",
    "    preprocess = hdfs_data.flatMap(parse_gzip_medline_str)\n",
    "    medline_df = preprocess.toDF()\n",
    "    \n",
    "    window = Window.partitionBy(['pmid']).orderBy(desc('file_name'))\n",
    "    ##only get the last version of documents\n",
    "    last_medline_df = medline_df.select(\n",
    "        max('delete').over(window).alias('is_deleted'),\n",
    "        rank().over(window).alias('pos'), '*').\\\n",
    "        where('is_deleted = False and pos = 1').\\\n",
    "        drop('is_deleted').drop('pos').drop('delete')\n",
    "    \n",
    "    if not call([\"hdfs\", \"dfs\", \"-test\", \"-d\", medline_parquet_path]):\n",
    "            logging.info('Parquet Files Exist Deleting .......')\n",
    "            call([\"hdfs\", \"dfs\", \"-rm\", \"-r\", \"-f\", medline_parquet_path])\n",
    "            \n",
    "    logging.info('Writing New parquet Files .......')\n",
    "    last_medline_df.write.parquet(medline_parquet_path)\n",
    "    spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
