{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/')\n",
    "import os, findspark\n",
    "os.environ['PYSPARK_PYTHON'] = '/home/tozeng/anaconda3/bin/python'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/home/tozeng/anaconda3/bin/python'\n",
    "import sys\n",
    "sys.path.append('/home/eileen/nsf_data_ingestion/')\n",
    "import zipfile\n",
    "import zipimport\n",
    "import io\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "from pyspark.sql import functions\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import rank, max, sum, desc\n",
    "import zlib\n",
    "import importlib\n",
    "import os\n",
    "from os import path\n",
    "from shutil import copyfile\n",
    "from shutil import rmtree\n",
    "from subprocess import call\n",
    "import pubmed_parser as pp\n",
    "import pyarrow as pa\n",
    "import subprocess\n",
    "\n",
    "#sys.path.append('/home/ananth/nsf_data_ingestion/')\n",
    "\n",
    "from nsf_data_ingestion.config import spark_config\n",
    "from nsf_data_ingestion.objects import data_source_params\n",
    "\n",
    "namenode_url = \"hdfs://ist-deacuna-n1.syr.edu:8020/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Creating Spark Session....\n"
     ]
    }
   ],
   "source": [
    "def create_session():\n",
    "    logging.info('Creating Spark Session....')\n",
    "    spark = SparkSession.builder.config(\"spark.executor.instances\", spark_config.exec_instance).\\\n",
    "                                 config(\"spark.executor.memory\", spark_config.exec_mem).\\\n",
    "                                 config('spark.executor.cores', spark_config.exec_cores).\\\n",
    "                                 config('spark.cores.max', spark_config.exec_max_cores).\\\n",
    "                                 config('spark.jars', '/home/eileen/nsf_data_ingestion/libraries/spark-xml_2.11-0.5.0.jar').\\\n",
    "                                 appName('medline_parquet_test').getOrCreate()\n",
    "    spark.sparkContext.addPyFile('/home/eileen/nsf_data_ingestion/libraries/pubmed_parser-0.1.0-py3.6.egg')\n",
    "    spark.sparkContext.addPyFile('/home/eileen/nsf_data_ingestion/libraries/Unidecode-1.1.1-py3.6.egg')\n",
    "#         logging.info('Adding Libraries' + str(library))\n",
    "#         spark.sparkContext.addPyFile(library)    # adding libraries\n",
    "#         spark.sparkContext.addPyFile(library)\n",
    "\n",
    "        \n",
    "    return spark\n",
    "spark = create_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/user/eileen/test/test/pubmed21n0001.xml.gz',\n",
       " '/user/eileen/test/test/pubmed21n0002.xml.gz',\n",
       " '/user/eileen/test/test/pubmed21n0003.xml.gz',\n",
       " '/user/eileen/test/test/pubmed21n0004.xml.gz',\n",
       " '/user/eileen/test/test/pubmed21n0005.xml.gz',\n",
       " '/user/eileen/test/test/pubmed21n0006.xml.gz',\n",
       " '/user/eileen/test/test/pubmed21n0007.xml.gz',\n",
       " '/user/eileen/test/test/pubmed21n0008.xml.gz',\n",
       " '/user/eileen/test/test/pubmed21n0009.xml.gz']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdfs = pa.hdfs.connect()\n",
    "file_paths = hdfs.ls('test/test')\n",
    "hdfs.ls('test/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd0 = sc.parallelize(list(range(10000))).repartition(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hostnode(e):\n",
    "    import sys\n",
    "    import socket\n",
    "    \n",
    "    return str(sys.version_info), str(socket.gethostname())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_version(e):\n",
    "    import pyarrow as pa\n",
    "    hdfs = pa.hdfs.connect()\n",
    "    \n",
    "    return str(pa.__version__), str(hdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 20.0 failed 4 times, most recent failure: Lost task 2.3 in stage 20.0 (TID 6578, ist-deacuna-n5.syr.edu, executor 53): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/rdd.py\", line 352, in func\n    return f(iterator)\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/rdd.py\", line 1861, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/lib/pyspark.zip/pyspark/shuffle.py\", line 238, in mergeValues\n    for k, v in iterator:\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-40-06a1b0e495c7>\", line 3, in check_version\n  File \"/home/tozeng/anaconda3/lib/python3.7/site-packages/pyarrow/hdfs.py\", line 207, in connect\n    extra_conf=extra_conf)\n  File \"/home/tozeng/anaconda3/lib/python3.7/site-packages/pyarrow/hdfs.py\", line 38, in __init__\n    self._connect(host, port, user, kerb_ticket, driver, extra_conf)\n  File \"pyarrow/io-hdfs.pxi\", line 89, in pyarrow.lib.HadoopFileSystem._connect\n  File \"pyarrow/error.pxi\", line 83, in pyarrow.lib.check_status\npyarrow.lib.ArrowIOError: Unable to load libhdfs\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1124)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:187)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/rdd.py\", line 352, in func\n    return f(iterator)\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/rdd.py\", line 1861, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/lib/pyspark.zip/pyspark/shuffle.py\", line 238, in mergeValues\n    for k, v in iterator:\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-40-06a1b0e495c7>\", line 3, in check_version\n  File \"/home/tozeng/anaconda3/lib/python3.7/site-packages/pyarrow/hdfs.py\", line 207, in connect\n    extra_conf=extra_conf)\n  File \"/home/tozeng/anaconda3/lib/python3.7/site-packages/pyarrow/hdfs.py\", line 38, in __init__\n    self._connect(host, port, user, kerb_ticket, driver, extra_conf)\n  File \"pyarrow/io-hdfs.pxi\", line 89, in pyarrow.lib.HadoopFileSystem._connect\n  File \"pyarrow/error.pxi\", line 83, in pyarrow.lib.check_status\npyarrow.lib.ArrowIOError: Unable to load libhdfs\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1124)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:187)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-809213624c8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrdd0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck_version\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistinct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    814\u001b[0m         \"\"\"\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 20.0 failed 4 times, most recent failure: Lost task 2.3 in stage 20.0 (TID 6578, ist-deacuna-n5.syr.edu, executor 53): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/rdd.py\", line 352, in func\n    return f(iterator)\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/rdd.py\", line 1861, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/lib/pyspark.zip/pyspark/shuffle.py\", line 238, in mergeValues\n    for k, v in iterator:\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-40-06a1b0e495c7>\", line 3, in check_version\n  File \"/home/tozeng/anaconda3/lib/python3.7/site-packages/pyarrow/hdfs.py\", line 207, in connect\n    extra_conf=extra_conf)\n  File \"/home/tozeng/anaconda3/lib/python3.7/site-packages/pyarrow/hdfs.py\", line 38, in __init__\n    self._connect(host, port, user, kerb_ticket, driver, extra_conf)\n  File \"pyarrow/io-hdfs.pxi\", line 89, in pyarrow.lib.HadoopFileSystem._connect\n  File \"pyarrow/error.pxi\", line 83, in pyarrow.lib.check_status\npyarrow.lib.ArrowIOError: Unable to load libhdfs\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1124)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:187)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/rdd.py\", line 352, in func\n    return f(iterator)\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/rdd.py\", line 1861, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/lib/pyspark.zip/pyspark/shuffle.py\", line 238, in mergeValues\n    for k, v in iterator:\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-40-06a1b0e495c7>\", line 3, in check_version\n  File \"/home/tozeng/anaconda3/lib/python3.7/site-packages/pyarrow/hdfs.py\", line 207, in connect\n    extra_conf=extra_conf)\n  File \"/home/tozeng/anaconda3/lib/python3.7/site-packages/pyarrow/hdfs.py\", line 38, in __init__\n    self._connect(host, port, user, kerb_ticket, driver, extra_conf)\n  File \"pyarrow/io-hdfs.pxi\", line 89, in pyarrow.lib.HadoopFileSystem._connect\n  File \"pyarrow/error.pxi\", line 83, in pyarrow.lib.check_status\npyarrow.lib.ArrowIOError: Unable to load libhdfs\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1124)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:187)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "rdd0.map(check_version).distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs = pa.hdfs.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyarrow.hdfs.HadoopFileSystem object at 0x7f6804f5bac8>\n"
     ]
    }
   ],
   "source": [
    "print(hdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"sys.version_info(major=3, minor=7, micro=3, releaselevel='final', serial=0)\",\n",
       "  'ist-deacuna-n6'),\n",
       " (\"sys.version_info(major=3, minor=7, micro=3, releaselevel='final', serial=0)\",\n",
       "  'ist-deacuna-n7'),\n",
       " (\"sys.version_info(major=3, minor=7, micro=3, releaselevel='final', serial=0)\",\n",
       "  'ist-deacuna-n4'),\n",
       " (\"sys.version_info(major=3, minor=7, micro=3, releaselevel='final', serial=0)\",\n",
       "  'ist-deacuna-n3'),\n",
       " (\"sys.version_info(major=3, minor=7, micro=3, releaselevel='final', serial=0)\",\n",
       "  'ist-deacuna-n2'),\n",
       " (\"sys.version_info(major=3, minor=7, micro=3, releaselevel='final', serial=0)\",\n",
       "  'ist-deacuna-n5'),\n",
       " (\"sys.version_info(major=3, minor=7, micro=3, releaselevel='final', serial=0)\",\n",
       "  'ist-deacuna-n9'),\n",
       " (\"sys.version_info(major=3, minor=7, micro=3, releaselevel='final', serial=0)\",\n",
       "  'ist-deacuna-n8')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd0.map(get_hostnode).distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"sys.version_info(major=3, minor=7, micro=3, releaselevel='final', serial=0)\",\n",
       " 'ist-deacuna-n1')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_hostnode(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.9\r\n"
     ]
    }
   ],
   "source": [
    "!/home/tozeng/anaconda3/bin/python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_p = spark.sparkContext.parallelize(file_paths)\n",
    "test_p.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_medline_parquet(gzip):\n",
    "    import pubmed_parser as pa\n",
    "    dicts = pa.parse_medline_xml(gzip[1])\n",
    "    return [Row(**doc) for doc in dicts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[abstract: string, affiliation: string, author: string, country: string, delete: boolean, issn_linking: string, journal: string, keywords: string, medline_ta: string, mesh_terms: string, nlm_unique_id: string, other_id: string, pmc: string, pmid: string, pubdate: string, title: string]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = spark.sparkContext.wholeTextFiles('test/test/pubmed21n0001.xml.gz')\n",
    "dicts = docs.flatMap(test_medline_parquet)\n",
    "dicts.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_gzip_medline_str(gzip_str):\n",
    "    import pubmed_parser as pp\n",
    "    import unidecode\n",
    "    \n",
    "    filepath = gzip_str[0]\n",
    "    xml_string = gzip_str[1]\n",
    "    _, file_name = path.split(filepath)\n",
    "    # decompress gzip\n",
    "\n",
    "    articles = pp.parse_medline_xml(xml_string)\n",
    "    return [Row(file_name=file_name, **article_dict)\n",
    "            for article_dict in articles]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(abstract='', affiliation='', author='AB Makar; KE McMartin; M Palese; TR Tephly', country='United States', delete=False, file_name='pubmed21n0001.xml.gz', issn_linking='0006-2944', journal='', keywords='', medline_ta='Biochem Med', mesh_terms='D000445:Aldehyde Oxidoreductases; D000818:Animals; D001826:Body Fluids; D002245:Carbon Dioxide; D005561:Formates; D000882:Haplorhini; D006801:Humans; D006863:Hydrogen-Ion Concentration; D007700:Kinetics; D000432:Methanol; D008722:Methods; D011549:Pseudomonas', nlm_unique_id='0151424', other_id='', pmc='', pmid='1', pubdate='1975', title='Formate assay in body fluids: application in methanol poisoning.'),\n",
       " Row(abstract='', affiliation='', author='KS Bose; RH Sarma', country='United States', delete=False, file_name='pubmed21n0001.xml.gz', issn_linking='0006-291X', journal='', keywords='', medline_ta='Biochem Biophys Res Commun', mesh_terms='D005583:Fourier Analysis; D009682:Magnetic Resonance Spectroscopy; D008958:Models, Molecular; D008968:Molecular Conformation; D009243:NAD; D009249:NADP; D013329:Structure-Activity Relationship; D013696:Temperature', nlm_unique_id='0372516', other_id='', pmc='', pmid='2', pubdate='1975', title='Delineation of the intimate details of the backbone conformation of pyridine nucleotide coenzymes in aqueous solution.'),\n",
       " Row(abstract='', affiliation='', author='RJ Smith; RG Bryant', country='United States', delete=False, file_name='pubmed21n0001.xml.gz', issn_linking='0006-291X', journal='', keywords='', medline_ta='Biochem Biophys Res Commun', mesh_terms='D000818:Animals; D001665:Binding Sites; D002104:Cadmium; D002256:Carbonic Anhydrases; D002417:Cattle; D006801:Humans; D006863:Hydrogen-Ion Concentration; D009682:Magnetic Resonance Spectroscopy; D008628:Mercury; D011485:Protein Binding; D011487:Protein Conformation; D015032:Zinc', nlm_unique_id='0372516', other_id='', pmc='', pmid='3', pubdate='1975', title='Metal substitutions incarbonic anhydrase: a halide ion probe study.'),\n",
       " Row(abstract='', affiliation='', author='UN Wiesmann; S DiDonato; NN Herschkowitz', country='United States', delete=False, file_name='pubmed21n0001.xml.gz', issn_linking='0006-291X', journal='', keywords='', medline_ta='Biochem Biophys Res Commun', mesh_terms='D001692:Biological Transport; D002478:Cells, Cultured; D002553:Cerebroside-Sulfatase; D002738:Chloroquine; D003911:Dextrans; D005347:Fibroblasts; D005966:Glucuronidase; D006801:Humans; D007966:Leukodystrophy, Metachromatic; D008247:Lysosomes; D010873:Pinocytosis; D012867:Skin; D013429:Sulfatases', nlm_unique_id='0372516', other_id='', pmc='', pmid='4', pubdate='1975', title='Effect of chloroquine on cultured fibroblasts: release of lysosomal hydrolases and inhibition of their uptake.'),\n",
       " Row(abstract='', affiliation='', author='WA Hendrickson; KB Ward', country='United States', delete=False, file_name='pubmed21n0001.xml.gz', issn_linking='0006-291X', journal='', keywords='', medline_ta='Biochem Biophys Res Commun', mesh_terms='D000818:Animals; D003063:Cnidaria; D003201:Computers; D006422:Hemerythrin; D008667:Metalloproteins; D008958:Models, Molecular; D009124:Muscle Proteins; D011487:Protein Conformation; D013045:Species Specificity', nlm_unique_id='0372516', other_id='', pmc='', pmid='5', pubdate='1975', title='Atomic models for the polypeptide backbones of myohemerythrin and hemerythrin.'),\n",
       " Row(abstract='', affiliation='', author='YW Chow; R Pietranico; A Mukerji', country='United States', delete=False, file_name='pubmed21n0001.xml.gz', issn_linking='0006-291X', journal='', keywords='', medline_ta='Biochem Biophys Res Commun', mesh_terms='D001665:Binding Sites; D003035:Cobalt; D006454:Hemoglobins; D006801:Humans; D006863:Hydrogen-Ion Concentration; D007501:Iron; D008024:Ligands; D008433:Mathematics; D010100:Oxygen; D010108:Oxyhemoglobins; D011485:Protein Binding; D013057:Spectrum Analysis', nlm_unique_id='0372516', other_id='', pmc='', pmid='6', pubdate='1975', title='Studies of oxygen binding energy to hemoglobin molecule.'),\n",
       " Row(abstract='', affiliation='', author='TR Anderson; TA Slotkin', country='England', delete=False, file_name='pubmed21n0001.xml.gz', issn_linking='0006-2952', journal='', keywords='', medline_ta='Biochem Pharmacol', mesh_terms='D000313:Adrenal Medulla; D000375:Aging; D000818:Animals; D000831:Animals, Newborn; D001835:Body Weight; D002395:Catecholamines; D004299:Dopamine beta-Hydroxylase; D004837:Epinephrine; D005260:Female; D006801:Humans; D066298:In Vitro Techniques; D008431:Maternal-Fetal Exchange; D008680:Metaraminol; D009020:Morphine; D009021:Morphine Dependence; D011247:Pregnancy; D051381:Rats; D014446:Tyrosine 3-Monooxygenase', nlm_unique_id='0101032', other_id='', pmc='', pmid='7', pubdate='1975', title='Maturation of the adrenal medulla--IV. Effects of morphine.'),\n",
       " Row(abstract='', affiliation='', author='K Moroi; T Sato', country='England', delete=False, file_name='pubmed21n0001.xml.gz', issn_linking='0006-2952', journal='', keywords='', medline_ta='Biochem Pharmacol', mesh_terms='D000581:Amidohydrolases; D000818:Animals; D004950:Esterases; D006863:Hydrogen-Ion Concentration; D066298:In Vitro Techniques; D007520:Isocarboxazid; D007700:Kinetics; D008297:Male; D008670:Metals; D008862:Microsomes, Liver; D010743:Phospholipids; D011343:Procaine; D011506:Proteins; D051381:Rats; D013347:Subcellular Fractions; D013696:Temperature', nlm_unique_id='0101032', other_id='', pmc='', pmid='8', pubdate='1975', title='Comparison between procaine and isocarboxazid metabolism in vitro by a liver microsomal amidase-esterase.'),\n",
       " Row(abstract='', affiliation='', author='J Marniemi; MG Parkki', country='England', delete=False, file_name='pubmed21n0001.xml.gz', issn_linking='0006-2952', journal='', keywords='', medline_ta='Biochem Pharmacol', mesh_terms='D000818:Animals; D002352:Carrier Proteins; D004852:Epoxy Compounds; D005978:Glutathione; D005982:Glutathione Transferase; D006863:Hydrogen-Ion Concentration; D008099:Liver; D008297:Male; D008748:Methylcholanthrene; D010634:Phenobarbital; D051381:Rats; D013268:Stimulation, Chemical; D013343:Styrenes', nlm_unique_id='0101032', other_id='', pmc='', pmid='9', pubdate='1975', title='Radiochemical assay of glutathione S-epoxide transferase and its enhancement by phenobarbital in rat liver in vivo.'),\n",
       " Row(abstract='', affiliation='', author='A Schmoldt; HF Benthe; G Haberland', country='England', delete=False, file_name='pubmed21n0001.xml.gz', issn_linking='0006-2952', journal='', keywords='', medline_ta='Biochem Pharmacol', mesh_terms='D000818:Animals; D002855:Chromatography, Thin Layer; D004073:Digitoxigenin; D004074:Digitoxin; D006900:Hydroxylation; D066298:In Vitro Techniques; D008297:Male; D008862:Microsomes, Liver; D009249:NADP; D051381:Rats; D013997:Time Factors', nlm_unique_id='0101032', other_id='', pmc='', pmid='10', pubdate='1975', title='Digitoxin metabolism by rat liver microsomes.')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = spark.sparkContext.wholeTextFiles('test/test/pubmed21n0001.xml.gz')\n",
    "dicts = docs.flatMap(parse_gzip_medline_str)\n",
    "dicts.take(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_medline_parquet(file_path):\n",
    "    import subprocess\n",
    "    (ret, out, err)= run_cmd(['hdfs', 'dfs', '-ls', 'test/test/'])\n",
    "    \n",
    "    return (out,err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cmd(args_list):\n",
    "        \"\"\"\n",
    "        run linux commands\n",
    "        \"\"\"\n",
    "        # import subprocess\n",
    "        print('Running system command: {0}'.format(' '.join(args_list)))\n",
    "        proc = subprocess.Popen(args_list, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        s_output, s_err = proc.communicate()\n",
    "        s_return =  proc.returncode\n",
    "        return s_return, s_output, s_err "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ret, out, err)= run_cmd(['hdfs', 'dfs', '-ls', 'test/test/'])\n",
    "out.decode('ascii').split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "err = test_p.map(test_medline_parquet).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err[0][1].decode('ascii').split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tet = spark.sparkContext.wholeTextFiles('test/test/*.xml.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tet.map(lambda x : x[0]).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa.hdfs.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
