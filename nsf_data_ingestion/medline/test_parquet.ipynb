{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/')\n",
    "import os, findspark\n",
    "os.environ['PYSPARK_PYTHON'] = '/home/tozeng/anaconda3/bin/python'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/home/tozeng/anaconda3/bin/python'\n",
    "import sys\n",
    "sys.path.append('/home/eileen/nsf_data_ingestion/')\n",
    "import zipfile\n",
    "import zipimport\n",
    "import io\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "from pyspark.sql import functions\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import rank, max, sum, desc\n",
    "import zlib\n",
    "import importlib\n",
    "import os\n",
    "from os import path\n",
    "from shutil import copyfile\n",
    "from shutil import rmtree\n",
    "from subprocess import call\n",
    "import pubmed_parser as pp\n",
    "import pyarrow as pa\n",
    "import subprocess\n",
    "\n",
    "#sys.path.append('/home/ananth/nsf_data_ingestion/')\n",
    "\n",
    "from nsf_data_ingestion.config import spark_config\n",
    "from nsf_data_ingestion.objects import data_source_params\n",
    "\n",
    "namenode_url = \"hdfs://ist-deacuna-n1.syr.edu:8020/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Creating Spark Session....\n"
     ]
    }
   ],
   "source": [
    "def create_session():\n",
    "    logging.info('Creating Spark Session....')\n",
    "    spark = SparkSession.builder.config(\"spark.executor.instances\", spark_config.exec_instance).\\\n",
    "                                 config(\"spark.executor.memory\", spark_config.exec_mem).\\\n",
    "                                 config('spark.executor.cores', spark_config.exec_cores).\\\n",
    "                                 config('spark.cores.max', spark_config.exec_max_cores).\\\n",
    "                                 config('spark.jars', '/home/eileen/nsf_data_ingestion/libraries/spark-xml_2.11-0.5.0.jar').\\\n",
    "                                 appName('medline_parquet_test').getOrCreate()\n",
    "    spark.sparkContext.addPyFile('/home/eileen/nsf_data_ingestion/libraries/pubmed_parser-0.1.0-py3.6.egg')\n",
    "    spark.sparkContext.addPyFile('/home/eileen/nsf_data_ingestion/libraries/Unidecode-1.1.1-py3.6.egg')\n",
    "#         logging.info('Adding Libraries' + str(library))\n",
    "#         spark.sparkContext.addPyFile(library)    # adding libraries\n",
    "#         spark.sparkContext.addPyFile(library)\n",
    "\n",
    "        \n",
    "    return spark\n",
    "spark = create_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/user/eileen/test/test/pubmed21n0001.xml.gz',\n",
       " '/user/eileen/test/test/pubmed21n0002.xml.gz',\n",
       " '/user/eileen/test/test/pubmed21n0003.xml.gz',\n",
       " '/user/eileen/test/test/pubmed21n0004.xml.gz',\n",
       " '/user/eileen/test/test/pubmed21n0005.xml.gz',\n",
       " '/user/eileen/test/test/pubmed21n0006.xml.gz',\n",
       " '/user/eileen/test/test/pubmed21n0007.xml.gz',\n",
       " '/user/eileen/test/test/pubmed21n0008.xml.gz',\n",
       " '/user/eileen/test/test/pubmed21n0009.xml.gz']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdfs = pa.hdfs.connect()\n",
    "file_paths = hdfs.ls('test/test')\n",
    "hdfs.ls('test/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_gzip_medline_str(gzip_str):\n",
    "    import pubmed_parser as pp\n",
    "    import unidecode\n",
    "    \n",
    "\n",
    "    filepath = gzip_str[0]\n",
    "    xml_string = gzip_str[1]\n",
    "    _, file_name = path.split(filepath)\n",
    "    # decompress gzip\n",
    "\n",
    "    articles = pp.parse_medline_xml(xml_string,nlm_category=True)\n",
    "    return [Row(file_name=file_name, **article_dict)\n",
    "            for article_dict in articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = sc.wholeTextFiles('test/test/pubmed21n0001.xml.gz').repartition(1000)\n",
    "dicts = docs.flatMap(parse_gzip_medline_str)\n",
    "df = dicts.toDF().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "abstract         object\n",
       "affiliation      object\n",
       "author           object\n",
       "country          object\n",
       "delete             bool\n",
       "file_name        object\n",
       "issn_linking     object\n",
       "journal          object\n",
       "keywords         object\n",
       "medline_ta       object\n",
       "mesh_terms       object\n",
       "nlm_unique_id    object\n",
       "other_id         object\n",
       "pmc              object\n",
       "pmid             object\n",
       "pubdate          object\n",
       "title            object\n",
       "dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "journal\n"
     ]
    }
   ],
   "source": [
    "for col in df.columns:\n",
    "    if (col != 'delete' and not \"\".join(df[col])):\n",
    "        print(col)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd0 = sc.parallelize(list(range(10000))).repartition(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hostnode(e):\n",
    "    import sys\n",
    "    import socket\n",
    "    \n",
    "    return str(sys.version_info), str(socket.gethostname())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_version(e):\n",
    "    import pyarrow as pa\n",
    "    hdfs = pa.hdfs.connect()\n",
    "    \n",
    "    return str(pa.__version__), str(hdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rdd0.map(check_version).distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs = pa.hdfs.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rdd0.map(get_hostnode).distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_hostnode(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/home/tozeng/anaconda3/bin/python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.1'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_p = spark.sparkContext.parallelize(file_paths)\n",
    "test_p.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_journal(gzip):\n",
    "    import re\n",
    "    l = re.findall('<Title>[\\s\\S]*?<\\/Title>', gzip[1])\n",
    "\n",
    "    return(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_medline_parquet(gzip):\n",
    "    import pubmed_parser as pa\n",
    "    dicts = pa.parse_medline_xml(gzip[1])\n",
    "    return [Row(**doc) for doc in dicts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = spark.sparkContext.wholeTextFiles('test/test/pubmed21n0001.xml.gz')\n",
    "dicts = docs.flatMap(find_journal)\n",
    "dicts.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_gzip_medline_str(gzip_str):\n",
    "    import pubmed_parser as pp\n",
    "    import unidecode\n",
    "    \n",
    "    filepath = gzip_str[0]\n",
    "    xml_string = gzip_str[1]\n",
    "    _, file_name = path.split(filepath)\n",
    "    # decompress gzip\n",
    "\n",
    "    articles = pp.parse_medline_xml(xml_string)\n",
    "    return [Row(file_name=file_name, **article_dict)\n",
    "            for article_dict in articles]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docs = spark.sparkContext.wholeTextFiles('test/test/pubmed21n0001.xml.gz')\n",
    "dicts = docs.flatMap(parse_gzip_medline_str)\n",
    "dicts.take(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_medline_parquet(file_path):\n",
    "    import subprocess\n",
    "    (ret, out, err)= run_cmd(['hdfs', 'dfs', '-ls', 'test/test/'])\n",
    "    \n",
    "    return (out,err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cmd(args_list):\n",
    "        \"\"\"\n",
    "        run linux commands\n",
    "        \"\"\"\n",
    "        # import subprocess\n",
    "        print('Running system command: {0}'.format(' '.join(args_list)))\n",
    "        proc = subprocess.Popen(args_list, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        s_output, s_err = proc.communicate()\n",
    "        s_return =  proc.returncode\n",
    "        return s_return, s_output, s_err "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ret, out, err)= run_cmd(['hdfs', 'dfs', '-ls', 'test/test/'])\n",
    "out.decode('ascii').split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "err = test_p.map(test_medline_parquet).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err[0][1].decode('ascii').split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tet = spark.sparkContext.wholeTextFiles('test/test/*.xml.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tet.map(lambda x : x[0]).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa.hdfs.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
